<!DOCTYPE html><html lang="en-US"><head><meta name="viewport" content="width=device-width"/><meta charSet="utf-8"/><title>Inference Attacks on Federated Learning - A Survey</title><meta property="og:type" content="article"/><meta property="og:title"/><meta property="og:description"/><meta property="og:image"/><meta name="twitter:card" content="summary_large_image"/><script>
            MathJax = {
              tex: {
                inlineMath: [['$', '$']]
              },
              svg: {
                fontCache: 'global'
              }
            };
            console.log("Loaded MathJax config");
        </script><script id="MathJax-script" async="" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script><meta name="next-head-count" content="10"/><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"/><link rel="manifest" href="/site.webmanifest"/><script>if (localStorage.theme === 'dark' || (!('theme' in localStorage) && window.matchMedia('(prefers-color-scheme: dark)').matches)) {           document.documentElement.classList.add('dark')             } else {           document.documentElement.classList.remove('dark')             }</script><link rel="preload" href="/_next/static/css/d35509232b5f14c5.css" as="style"/><link rel="stylesheet" href="/_next/static/css/d35509232b5f14c5.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-cb7634a8b6194820.js" defer=""></script><script src="/_next/static/chunks/framework-e70c6273bfe3f237.js" defer=""></script><script src="/_next/static/chunks/main-a054bbf31fb90f6a.js" defer=""></script><script src="/_next/static/chunks/pages/_app-3638d23458ed6d8c.js" defer=""></script><script src="/_next/static/chunks/29107295-f29cbf9716887c91.js" defer=""></script><script src="/_next/static/chunks/123-2d5a47533d991588.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-301c471cd34d9b12.js" defer=""></script><script src="/_next/static/YZIEIcGJBIygEvu0QOl3a/_buildManifest.js" defer=""></script><script src="/_next/static/YZIEIcGJBIygEvu0QOl3a/_ssgManifest.js" defer=""></script><script src="/_next/static/YZIEIcGJBIygEvu0QOl3a/_middlewareManifest.js" defer=""></script></head><body><div id="__next" data-reactroot=""><div class="flex flex-col min-h-screen"><header id="TOP" class="py-4 px-8"><div class="container max-w-3xl mx-auto flex justify-between"><a class="font-mono no-underline" href="/">~/</a><div class="font-mono my-auto text-accent-1-light dark:text-accent-1-dark"><span class="hidden sm:block ">cochaviz@bunkernet.cc</span> <span class="block sm:hidden ">cochaviz</span> </div><div class="flex gap-x-2"><a href="https://www.github.com/cochaviz"> github </a><a href="https://www.linkedin.com/in/cochaviz"> linkedin </a><button id="dark-mode-toggle-light" class="border-background-alter-light dark:border-background-alter-dark border-2 px-2 hidden dark:block">light mode</button><button id="dark-mode-toggle-dark" class="border-background-alter-light dark:border-background-alter-dark border-2 px-2 block dark:hidden">dark mode</button></div></div></header><main class="container max-w-2xl"><div><a class="text-5xl font-sans no-underline fixed bottom-5 right-5 sm:bottom-10 sm:right-10 z-50 bg-background-light dark:bg-background-dark border-border-light dark:border-border-dark px-3 pb-2 border-double border-4" href="#TOP">↑</a><h1>Inference Attacks on Federated Learning - A Survey</h1><abstract><h3>Abstract</h3> <p>Federated Learning (FL) is a privacy-preserving approach to distributed machine learning, but it is vulnerable to inference attacks. Inference attacks aim to extract information about regarding model or data used in FL, directly threatening one of its core principles. This essay provides an overview of state-of-the-art inference attacks in FL and their implications for data privacy. It introduces the basics of FL, different types of inference attacks (model inversion, membership inference, property inference), and provides an overview of recent research on gradient inversion and membership inference attacks. We emphasize the need for robust defenses to safeguard sensitive information in FL systems, along with the importance of future research in addressing these increasingly realistic threats.</p></abstract><article><h2 id="introduction" tabindex="-1">Introduction</h2>
<p>Machine learning models have demonstrated remarkable capabilities in
interpreting and deriving insights from data, leading to significant
advancements in fields such as medical diagnostics (Kononenko, 2001) and
natural language processing (Liu et al., 2023). However, these
applications often involve handling privacy-sensitive data (Rieke et
al., 2020), which can be inferred from trained models (Fredrikson et
al., 2015), raising concerns about data privacy and security. Federated
Learning is a privacy-preserving approach to distributed machine
learning that aims to address these concerns (McMahan &amp; Ramage, 2017)
but is not immune to potential exploits (Abad et al., 2022). Inference
Attacks directly threaten this privacy-preserving feature by attempting
to extract information about the model and the data it was trained on
(Geiping et al., 2020).</p>
<p>In this essay, I provide an overview of state-of-the-art Inversion
Attacks and their defenses to Federated Learning. Besides informing on
the state-of-the-art, this essay should provide an introduction to the
subject for both machine learning and cyber security specialists. First,
we will discuss the basics of Federated Learning, introduce attacks on
Machine Learning models, and consider a taxonomy of Inference Attacks on
Federated Learning. Then, I will present novel attacks and some of their
defenses. Finally, we will discuss the threat these attacks present when
considering the defenses and present future work to accommodate for
these threats.</p>
<h2 id="background" tabindex="-1">Background</h2>
<p>In this section, the necessary background information will be
introduced. The background is considered whatever already existed until
last year (March 2022). We will provide a concise overview of machine
learning principles, to then discuss the workings of Federated Learning
(FL). Having covered the necessary machine learning knowledge, the
discussion will move to how one would attack such systems. Finally, we
focus on previous inference attacks as summarized and discussed by (Abad
et al., 2022).</p>
<h3 id="machine-learning" tabindex="-1">Machine Learning</h3>
<p>The goal of any machine learning algorithm is to predict some label or
value given familiar but unseen data. For the purposes of this
discussion, the machine-learning process can be separated into 3 stages:</p>
<ol>
<li>Training</li>
<li>Testing/Evaluation</li>
<li>Deployment</li>
</ol>
<p>During training, the machine learning model, $f$ is given a set of
tuples ${(x_i, y_i)}$. The learning algorithm then adjusts the model
parameters, $\theta$, such that the model after training, $f_\theta$,
maps the input features $x$ to the target value(s) $y$. Depending on the
learning task, $y$ could be a continuous value (regression), a binary
value (binary classification), or a set of discrete values (multi-class
classification)<sup class="footnote-ref"><a href="#fn1" id="fnref1">1</a></sup> (Abad et al., 2022; Chakraborty et al., 2018).</p>
<p>The testing phase assesses the performance of the model. We take a
similar, but unseen set of tuples ${(x, y)}$ and test whether the
model, $f_\theta(x)$, returns the correct value(s). It’s important to
only test on <em>unseen</em> data since the aim is to assess the <em>generalizing</em>
ability of the model. One can imagine the performance to be higher if
the same data that was used to train, was used to test.</p>
<p>After the model is trained and evaluated, it is then deployed. Often
accessed via a public or private API. In the case of Federated Learning,
this process is iterative.</p>
<h3 id="federated-learning" tabindex="-1">Federated Learning</h3>
<p>Federated Learning (FL) is a method of delegating, or democratizing, the
training stage of a machine learning algorithm. Its benefits are
threefold (Konečný et al., 2017):</p>
<ol>
<li>It avoids sharing <em>raw</em> personal data with a third party.</li>
<li>Processing resources are delegated.</li>
<li>Data that is fragmented over multiple locations can still be used
for training.</li>
</ol>
<p>The process, generally, works as follows. Each client in set of clients,
$C={c_0, \ldots, c_n}$, trains a private machine learning model their
respective dataset. Information about this trained model is then sent to
a central server that <em>aggregates</em> the information from all clients into
a single model. The newly trained model is then sent back to the clients
for another iteration<sup class="footnote-ref"><a href="#fn2" id="fnref2">2</a></sup> Konečný et al. (2017).</p>
<figure>
<img src="/images/post/inference-attacks-on-federated-learning/client-server-fl.svg"
alt="Typical Federated Learning network topology. The client, c_i, sends the gradient, \nabla Q(\theta_i), and/or weights, \theta_i, of a particular iteration i. The central server then sends the updated model parameters \theta_{i+1} initiating the next iteration." />
<figcaption aria-hidden="true">Typical Federated Learning network
topology. The client, <span
class="math inline"><em>c</em><sub><em>i</em></sub></span>, sends the
gradient, <span
class="math inline">∇<em>Q</em>(<em>θ</em><sub><em>i</em></sub>)</span>,
and/or weights, <span
class="math inline"><em>θ</em><sub><em>i</em></sub></span>, of a
particular iteration <span class="math inline"><em>i</em></span>. The
central server then sends the updated model parameters <span
class="math inline"><em>θ</em><sub><em>i</em> + 1</sub></span>
initiating the next iteration.</figcaption>
</figure>
<p>Various aggregation algorithms exist. The most popular of which are
<em>Federated Stochastic Gradient Descent</em> (FedSGD) and <em>Federated
Averaging</em> (FedAvg). These use the <em>gradient</em> (the function that
describes how to optimize the model) or the aforementioned parameters,
$\theta$, of the client models respectively when communicating model
updates Gu et al. (2022). While technical details of these algorithms
are not crucial to this discussion, it is important to note that both
the gradient and the weights contain embedded information about the
client’s dataset (Fredrikson et al., 2015; Geiping et al., 2020).</p>
<h4 id="vertical-versus-horizontal-fl" tabindex="-1">Vertical versus Horizontal FL</h4>
<p>Lastly, there are two types of FL: Horizontal Federated Learning (HFL),
or cross-device Federated Learning; and Vertical Federated Learning
(VFL), or cross-silo Federated Learning (Abad et al., 2022; Suri et al.,
2022). These differentiate in the way the client data is aligned to
provide a trainable model.</p>
<p>In the former, devices all collect data on the same features, but their
sample space is not equal (the distributions might not align, and the
size can be different). This is the most used type of Federated
Learning, which also is how companies such as Google train their models
on user data (McMahan &amp; Ramage, 2017).</p>
<p>In the latter, we can imagine a set of hospitals or companies (or <em>data
silos</em>) that need to train a model on <em>all</em> the available user data. The
data, however, cannot be directly shared and the kind of data they
collect on their users is also different. Regardless, each user, or
<em>subject</em>, is present in each database. They thus share the same sample
space but differ in the features they train their local models on.</p>
<h3 id="attacking-machine-learning" tabindex="-1">Attacking Machine Learning</h3>
<p>The machine learning approaches discussed so far (normal/<em>centralized</em>
learning and Federated Learning) contain several points at which an
attacker could intervene to exploit various characteristics of the
system. Before discussing inference attacks that take place in later
stages of the machine learning pipeline, let us briefly discuss other
potential threats to machine learning models.</p>
<p>The phases discussed in the first section (training, testing, and
deployment) correspond directly to different attacks which can be
categorized as follows (Chakraborty et al., 2018).</p>
<ol>
<li>
<p><em>Poisoning Attack</em>: This type of attack, known as contamination of
the training data, takes place during the training time of the
machine learning model. An adversary tries to poison the training
data by injecting carefully designed samples to compromise the whole
learning process eventually.</p>
</li>
<li>
<p><em>Evasion Attack</em>: This is the most common type of attack in the
adversarial setting. The adversary tries to evade the system by
adjusting malicious samples during the testing phase. This setting
does not assume any influence over the training data.</p>
</li>
<li>
<p><em>Inference/Exploratory Attack</em>: These attacks do not influence any
dataset. Given black-box access to the model, they try to gain as
much knowledge as possible about the learning algorithm of the
underlying system and pattern in training data.</p>
</li>
</ol>
<p>While the first two also pose potential threats to the FL scheme and are
very popular in centralized machine learning, they are considerably
harder to perform on Federated Learning as the data is distributed
(Tolpegin et al., 2020). Databases of multiple clients have to be
compromised to create an exploit that is comparable to that of a
centralized machine-learning approach.</p>
<p>Inference attacks, however, threaten the privacy guarantees FL attempts
to give. Inference attacks specifically try to <em>infer</em> information about
the dataset the model was trained on or the model itself. Thereby
threatening the confidentiality of the database, and thus the privacy,
of the victims involved (Abad et al., 2022). Since they actively infer
information about a deployed system, the amount of information on the
system determines how powerful such an attack could be. For this reason,
they are also further specified as <em>white-box</em> or <em>black-box</em>, and
sometimes <em>grey-box</em> inference attacks (Nasr et al., 2019).</p>
<h3 id="inference-attacks" tabindex="-1">Inference Attacks</h3>
<p>Inference attacks can be applied to both centralized machine learning
models and Federated Learning schemes. Many of the principles we will
cover apply to both centralized and Federated Learning, but the focus
will be on applications on FL. Specifically, we will provide an overview
of attack classifications as given by Abad et al. (2022).</p>
<p>Firstly, depending on the target information the attacker attempts to
infer, the attack is classified as follows:</p>
<ul>
<li>
<p><em>Model Inversion</em>: In model inversion, the attacker attempts to invert
the machine learning model. Thereby finding the data point
corresponding to a certain label. Fredrikson et al. (2015) were able
to invert a facial recognition model, allowing them to recover the
image of an individual known to be in the training dataset.</p>
</li>
<li>
<p><em>Membership Inference</em>: In this attack, the goal is to determine
whether a data point $(x, y)$ was part of the training set. In FL it
is also possible to determine whether a data point was part of the
training set of a particular client.</p>
</li>
<li>
<p><em>Property Inference</em>: Property inference attempts to detect whether
the dataset used to train the (local) model contains some property
(e.g. whether images are noisy) (Ganju et al., 2018).</p>
</li>
</ul>
<p>Secondly, when considering a malicious central server, the attack can be
classified according to the manner in which the attacker interferes with
the training procedure (Abad et al., 2022):</p>
<ul>
<li>
<p><em>Passive</em>: Also known as an <em>honest-but-curious</em> scenario. The
attacker can only read or eavesdrop on communications (i.e. the
weights and gradient transmitted between the clients and the server),
and the local model and dataset in case of an honest-but-curious
client.</p>
</li>
<li>
<p><em>Active</em>: The attack is more effective than the former but less
stealthy. It essentially changes the learning goal from minimizing
loss to maximizing inferred information from the victim.</p>
</li>
</ul>
<p>Lastly, attacks can be categorized based on the position of the attacker
in the network:</p>
<ul>
<li>
<p><em>Local</em>: The attacker is a client, i.e. they can only access <em>their</em>
database, parameters, and the global parameters they receive from the
server.</p>
</li>
<li>
<p><em>Global</em>: The attacker is the central server. They do not have access
to any databases but can access the gradients/parameters sent by all
clients and the global model.</p>
</li>
</ul>
<h2 id="inference-attacks-in-federated-learning" tabindex="-1">Inference Attacks in Federated Learning</h2>
<p>This section will discuss the state-of-the-art of inference attacks on
Federated Learning. Specifically, we will discuss progress in the field
as of March 2022. The research presented here was found primarily by
querying Google Scholar with the terms “Inference Attacks on Federated
Learning”, “Membership Inference on Federated Learning”, “Model
Inversion on Federated Learning”, and “Gradient Inversion on Federated
Learning”. The next section will cover the threats these advances pose
to current systems.</p>
<p>First, we will discuss various attacks, focusing not only on their
performance but paying special attention to the scenario in which the
researchers placed the hypothetical adversary. Then, we will cover
defenses to some of these attacks.</p>
<h3 id="attacking" tabindex="-1">Attacking</h3>
<p>Various types of attacks fall under the umbrella of inference attacks.
As of the writing of this essay, the most popular are <em>Membership
Inference</em> and <em>Gradient Inversion</em> as these show the most results.
<em>Passive</em> inference attacks are more often covered than their <em>active</em>
counterparts. For each paper, we annotate the type of attack (see
<a href="#inference-attacks">Inference Attacks</a>), summarize the findings of the
authors, and briefly discuss them.</p>
<h4 id="do-gradient-inversion-attacks-make-federated-learning-unsafe%3F" tabindex="-1">Do Gradient Inversion Attacks Make Federated Learning Unsafe?</h4>
<p>Keywords: <em>Model Inversion</em>, <em>Local/Global</em>, <em>Cross-Device/HFL</em>,
<em>Passive</em></p>
<p>Hatamizadeh et al. (2023) performed image reconstruction using gradient
inversion while relaxing a strong assumption made in prior work
regarding Batch Normalization (BN) (Ioffe &amp; Szegedy, 2015). BN is a
technique used in neural networks that significantly improve the
learning rate and stability, and is therefore ubiquitous in modern
machine learning. The technique introduces two learned parameters,
$\beta$ and $\gamma$, which thus change during the learning
process(Ioffe &amp; Szegedy, 2015). Previous work has assumed these
statistics to be static (Geiping et al., 2020; Kaissis et al., 2021),
introducing an error that would compound over time. The authors were
able to reliably reconstruct images without assuming static BN
statistics. The authors make a strong case for an inversion attack that
could be used in practice but still rely on priors (approximations of
the image) to make accurate reconstructions.</p>
<h4 id="improved-gradient-inversion-attacks-and-defenses-in-federated-learning" tabindex="-1">Improved Gradient Inversion Attacks and Defenses in Federated Learning</h4>
<p>Keywords: <em>Membership Inference</em>, <em>Local/Global</em>, <em>Cross-Device/HFL</em>,
<em>Passive</em>, <em>White-Box</em></p>
<p>Geng et al. (2023) proposed a framework for inverting both
<em>FedAVG</em>-based and <em>FedSGD</em>-based networks in an “honest-but-curious”
scenario. They mention prior work has failed to effectively perform
gradient inversion when FL uses the <em>FedAVG</em> aggregation algorithm.
Furthermore, they specify methods for fine-tuning the performance of
image restoration in the inverted model, allowing them to restore images
that were introduced 16 epochs before the current iteration. As
Federated Learning is an iterative process, one can imagine that the
further a data point is removed from the current iteration, the harder
it is to infer from the current gradient. While their results are
promising, they do assume a white-box attack scenario making their
attack harder to perform.</p>
<h4 id="cs-mia%3A-membership-inference-attack-based-on-prediction-confidence-series-in-federated-learning" tabindex="-1">CS-MIA: Membership Inference Attack Based on Prediction Confidence Series in Federated Learning</h4>
<p>Keywords: <em>Membership Inference</em>, <em>Local/Global</em>, <em>Cross-Device/VFL</em>,
<em>Passive</em></p>
<p>Gu et al. (2022) were able to determine whether data points are members
of certain datasets by following the trend in their classification
confidence. Over time, the global model should perform less well on
participants’ private data, meaning that member data should follow a
different trend compared to non-member data. They then train a
supervised model the determine whether data points were part of the
training set based on this assumption. The model is then used to
determine the probability of unseen data being part of the target
training data set. They show high accuracy and F1-scores for all
datasets with the lowest performer being MNIST (around 60% compared to
&gt;90% for the other datasets). Still, the proposed solution scores the
best out of all included approaches by a significant margin.</p>
<h4 id="subject-membership-inference-attacks-in-federated-learning" tabindex="-1">Subject Membership Inference Attacks in Federated Learning</h4>
<p>Keywords: <em>Membership Inference</em>, <em>Local/Global</em>, <em>Cross-Silo/VFL</em>,
<em>Passive</em></p>
<p>In a black-box setting, Suri et al. (2022) propose a method for what
they call <em>Subject Inference</em> (see <a href="#federated-learning">Federated
Learning</a>). They describe previous work as being
disconnected from real-world scenarios as it (i) includes information
adversaries would not normally have access to and (ii) assumes the
adversary is looking for data points rather than individuals. Instead of
determining whether one particular data point was part of the training
set, the authors attempt to infer whether an individual, a <em>subject</em>,
(or rather their distribution) is present in the dataset given some
preexisting information on them. They show the attack to be very
effective in various real-world datasets while also increasing the
realism of the scenario. They show Subject Inference to be a real threat
to user privacy.</p>
<h4 id="active-membership-inference-attack-under-local-differential-privacy-in-federated-learning." tabindex="-1">Active Membership Inference Attack under Local Differential Privacy in Federated Learning.</h4>
<p>Keywords: <em>Membership Inference</em>, <em>Local/Global</em>, <em>Cross-Device/HFL</em>,
<em>Active</em></p>
<p>Different from other works, Nguyen et al. (2023) considers a maximally
malicious, i.e. <em>active</em>, membership inference attack. They implement a
method for inferring membership of a particular data point in the
presence of differential privacy (Dwork &amp; Roth, 2013). Differential
privacy obscures the relation of the individual to the data point,
without affecting the patterns used for training machine learning
models. The authors show that their method performs well, even under
such obscuring of the data. Furthermore, the attack only starts to
degrade after the level of obscurity interferes with model performance.
They show that more rigorous privacy methods should be proposed to deal
with such attacks.</p>
<h3 id="defending" tabindex="-1">Defending</h3>
<p>To combat inference attacks, we discuss potential defenses against them.
Some of the papers that are included have been discussed in the last
section. These have been marked with a footnote accordingly <sup class="footnote-ref"><a href="#fn3" id="fnref3">3</a></sup>. For
each paper, we will summarize the proposed measures and briefly discuss
them.</p>
<h4 id="improved-gradient-inversion-attacks-and-defenses-in-federated-learning-1" tabindex="-1">Improved Gradient Inversion Attacks and Defenses in Federated Learning<sup class="footnote-ref"><a href="#fn4" id="fnref4">4</a></sup></h4>
<p>Geng et al. (2023) found that labels that only appeared only once were
more prone to their proposed inversion attacks (see
<a href="#improved-gradient-inversion-attacks-and-defenses-in-federated-learning">Attacks</a>).
They also mention the use of larger batch sizes in the global model
(i.e. more clients) to reduce the amount of private information embedded
in a single batch. Lastly, they claim FedAVG possesses “stronger privacy
preserving capabilities than FedSGD”. As this was included in the
discussion of their attack-oriented paper, they do not evaluate these
claims further.</p>
<h4 id="do-gradient-inversion-attacks-make-federated-learning-unsafe%3F-1" tabindex="-1">Do Gradient Inversion Attacks Make Federated Learning Unsafe?<sup class="footnote-ref"><a href="#fn5" id="fnref5">5</a></sup></h4>
<p>Hatamizadeh et al. (2023) make several recommendations to make existing
implementations of FL safer, namely: (i) larger training sets, (ii)
updates from a larger number of iterations over different (iii) large
batch sizes. In addition, they mention three more changes that could
potentially mitigate server-side (i.e. <em>Global</em>) gradient inversion
attacks: (1) The use of <em>Homomorphic Encryption</em> (see <a href="#future-work">Future
Work</a>), (2) ensuring the attacker does not have knowledge
of the model architecture, and (3) using an alternative aggregation
algorithm such as FedBN Andreux et al. (2020). The countermeasures
provided are relatively general. They also provided sources affirming
their suspicions.</p>
<h4 id="an-empirical-analysis-of-image-augmentation-against-model-inversion-attack-in-federated-learning" tabindex="-1">An Empirical Analysis of Image Augmentation Against Model Inversion Attack in Federated Learning</h4>
<p>Shin et al. (2023) propose the use of image augmentation as a more
viable alternative to differential privacy (Dwork &amp; Roth, 2013). Image
augmentation is a data synthesis method that increases the size of the
training set, and reduces over-fitting (Shorten &amp; Khoshgoftaar, 2019).
As this introduces fake data while improving the overall performance of
the model, the authors suggest it could be used to mitigate model
inversion attacks. The attack they used was introduced by (Geiping et
al., 2020), and various more successful attacks have been constructed
since then Geng et al. (2023).</p>
<h4 id="ressfl%3A-a-resistance-transfer-framework-for-defending-model-inversion-attack-in-split-federated-learning" tabindex="-1">ResSFL: A Resistance Transfer Framework for Defending Model Inversion Attack in Split Federated Learning</h4>
<p>In a framework introduced by (J. Li et al., 2022), Split Federated
Learning (SFL) (Annavaram &amp; Avestimehr, n.d.) is augmented with a model
that attempts to invert the model before the client sends their model to
the central server. By choosing weights where the discriminator performs
poorly, they claim to improve the resiliency of the scheme to model
inversion attacks. They indeed show improvements over the standard
implementation of SFL but do not mention how this method compares to
attacks on default FL.</p>
<h2 id="discussion" tabindex="-1">Discussion</h2>
<p>In this section, we will discuss the attacks and defenses as presented
in the last section. Specifically, we determine the threats these
attacks pose and if the defenses included could effectively mitigate
the. In the end, we propose new research directions that could help
mitigate these threats.</p>
<h3 id="current-threats-and-trends" tabindex="-1">Current Threats and Trends</h3>
<p>The attacks presented show how Federated Learning might not be able to
guarantee privacy. Privacy, thus, should still remain a concern even if
Federated Learning brings stronger privacy guarantees than traditional
machine learning and its derivatives. Let us summarize the threats these
attacks pose:</p>
<ul>
<li>
<p><em>More Realistic Scenarios</em>: Research starts to introduce more
realistic scenarios that could threaten current implementations of
Federated Learning. As the field matures, attacks seem to become more
realistic. Especially the work presented by Suri et al. (2022) poses a
real threat as it assumes a complete black-box attack with reasonable
assumptions while still showing good performance. Even in complete
black-box settings, however, we still assume the ability to intercept
and read the communications. Were this to be encrypted, such attacks
could possibly be mitigated (Y. Li et al., 2021).</p>
</li>
<li>
<p><em>Increased Resilience Against Existing Privacy Measures</em>: Some of the
aforementioned papers has shown improvements concerning the evasion of
privacy-preserving measures. Nguyen et al. (2023) have shown how a
membership inference attack can be effectively performed in the
presence of differential privacy. Their method was effective to such a
degree that the attack was ineffective only once the privacy measures
started to affect model performance. The image augmentation
countermeasure proposed by (Shin et al., 2023) could be a viable
option. This countermeasure, however, was only tested in a <em>passive</em>
scenario.</p>
</li>
<li>
<p><em>Stronger Attacks in Existing Scenarios</em>: As to be expected, some work
was focused on improving performance in existing scenarios. Gu et
al. (2022) have shown that much is still to be learned in the field by
proposing a relatively simple approach that improves upon all previous
methods by a large margin.</p>
</li>
</ul>
<p>Such developments are not surprising, progress in both offense and
defense is to be expected. The speed at which research moves forward is
very impressive and suggests the field is still in the early stages of
development. When considering using such new technologies in production,
this could be considered when assessing the security of such systems.</p>
<h3 id="future-work" tabindex="-1">Future Work</h3>
<p>Considering the aforementioned advances, the following directions could
provide useful for future research:</p>
<ol>
<li>
<p>Consider using existing preprocessing methods for privacy
preservation. Shin et al. (2023) and Hatamizadeh et al. (2023) both
either use or suggest using existing pre-processing or other
learning-enhancing augmentations to improve privacy. Efforts toward
generalizing data <em>before</em> training might prove a solution to both
overfitting and privacy.</p>
</li>
<li>
<p>New attack methods would benefit from relaxing assumptions instead
of attempting to increase performance. By doing this, various of the
attacks shown have been to provide a more realistic view of the
privacy-preserving features of FL. While performance improvements
might provide interesting results and insights, focusing efforts on
exposing potential <em>realistic</em> threats would have a more direct
effect on our ability to assess FL from a privacy perspective.</p>
</li>
<li>
<p>Working, safe Homomorphic Encryption (HE) would hamper most of the
aforementioned attacks. Being able to encrypt data <em>before</em> training
a model would make inference attacks completely benign (Lee et al.,
2022). Work from the past year, however, was able to infer
privacy-sensitive information about the training set regardless of
the presence of HE (Y. Li et al., 2021). More research on the robust
use of HE could prove a catch-all solution for many of the presented
machine-learning attacks.</p>
</li>
</ol>
<h2 id="conclusion" tabindex="-1">Conclusion</h2>
<p>This essay has provided an overview for security specialists and machine
learning specialists to assess the current state of Inference Attacks in
Federated Learning. Progress over the last year has shown the field to
be advancing quickly. Introducing successful attacks on new, more
realistic scenarios, showing the ability to circumvent mature
privacy-preserving, measures and improving the performance of existing
methods. The developments seen here are concerning given the prevalence
of Federated Learning. More research is needed to assess how
privacy-preserving Federated Learning actually is and whether additional
countermeasures provide enough security to circumvent the apparent
threats presented here.</p>
<h2 id="references" tabindex="-1">References</h2>
<div id="refs" class="references csl-bib-body hanging-indent"
line-spacing="2">
<div id="ref-abadSecurityPrivacyFederated2022" class="csl-entry">
<p>Abad, G., Picek, S., Ramírez-Durán, V. J., &amp; Urbieta, A. (2022). <em>On the
Security &amp; Privacy in Federated Learning</em> (No. arXiv:2112.05423). arXiv.
<a href="https://arxiv.org/abs/2112.05423">https://arxiv.org/abs/2112.05423</a></p>
</div>
<div id="ref-andreuxSiloedFederatedLearning2020" class="csl-entry">
<p>Andreux, M., Du Terrail, J. O., Beguier, C., &amp; Tramel, E. W. (2020).
Siloed Federated Learning for <span class="nocase">Multi-centric
Histopathology Datasets</span>. In S. Albarqouni, S. Bakas, K.
Kamnitsas, M. J. Cardoso, B. Landman, W. Li, F. Milletari, N. Rieke, H.
Roth, D. Xu, &amp; Z. Xu (Eds.), <em>Domain Adaptation and Representation
Transfer, and Distributed and Collaborative Learning</em> (Vol. 12444, pp.
129–139). Springer International Publishing.
<a href="https://doi.org/10.1007/978-3-030-60548-3_13">https://doi.org/10.1007/978-3-030-60548-3_13</a></p>
</div>
<div id="ref-annavaramGroupKnowledgeTransfer" class="csl-entry">
<p>Annavaram, C. H. M., &amp; Avestimehr, S. (n.d.). <em>Group Knowledge Transfer:
Federated Learning of Large CNNs at the Edge</em>.</p>
</div>
<div id="ref-chakrabortyAdversarialAttacksDefences2018"
class="csl-entry">
<p>Chakraborty, A., Alam, M., Dey, V., Chattopadhyay, A., &amp; Mukhopadhyay,
D. (2018). <em>Adversarial Attacks and Defences: A Survey</em> (No.
arXiv:1810.00069). arXiv. <a href="https://arxiv.org/abs/1810.00069">https://arxiv.org/abs/1810.00069</a></p>
</div>
<div id="ref-dworkAlgorithmicFoundationsDifferential2013"
class="csl-entry">
<p>Dwork, C., &amp; Roth, A. (2013). The Algorithmic Foundations of
Differential Privacy. <em>Foundations and Trends in Theoretical Computer
Science</em>, <em>9</em>(3-4), 211–407. <a href="https://doi.org/10.1561/0400000042">https://doi.org/10.1561/0400000042</a></p>
</div>
<div id="ref-fredriksonModelInversionAttacks2015" class="csl-entry">
<p>Fredrikson, M., Jha, S., &amp; Ristenpart, T. (2015). Model Inversion
Attacks that Exploit Confidence Information and Basic Countermeasures.
<em>Proceedings of the 22nd ACM SIGSAC Conference on Computer and
Communications Security</em>, 1322–1333.
<a href="https://doi.org/10.1145/2810103.2813677">https://doi.org/10.1145/2810103.2813677</a></p>
</div>
<div id="ref-ganjuPropertyInferenceAttacks2018" class="csl-entry">
<p>Ganju, K., Wang, Q., Yang, W., Gunter, C. A., &amp; Borisov, N. (2018).
Property Inference Attacks on Fully Connected Neural Networks using
Permutation Invariant Representations. <em>Proceedings of the 2018 ACM
SIGSAC Conference on Computer and Communications Security</em>, 619–633.
<a href="https://doi.org/10.1145/3243734.3243834">https://doi.org/10.1145/3243734.3243834</a></p>
</div>
<div id="ref-geipingInvertingGradientsHow2020" class="csl-entry">
<p>Geiping, J., Bauermeister, H., Dröge, H., &amp; Moeller, M. (2020).
Inverting Gradients - How easy is it to break privacy in federated
learning? <em>Advances in Neural Information Processing Systems</em>, <em>33</em>,
16937–16947.</p>
</div>
<div id="ref-gengImprovedGradientInversion2023" class="csl-entry">
<p>Geng, J., Mou, Y., Li, Q., Li, F., Beyan, O., Decker, S., &amp; Rong, C.
(2023). Improved Gradient Inversion Attacks and Defenses in Federated
Learning. <em>IEEE Transactions on Big Data</em>, 1–13.
<a href="https://doi.org/10.1109/TBDATA.2023.3239116">https://doi.org/10.1109/TBDATA.2023.3239116</a></p>
</div>
<div id="ref-guCSMIAMembershipInference2022" class="csl-entry">
<p>Gu, Y., Bai, Y., &amp; Xu, S. (2022). CS-MIA: Membership inference attack
based on prediction confidence series in federated learning. <em>Journal of
Information Security and Applications</em>, <em>67</em>, 103201.
<a href="https://doi.org/10.1016/j.jisa.2022.103201">https://doi.org/10.1016/j.jisa.2022.103201</a></p>
</div>
<div id="ref-hatamizadehGradientInversionAttacks2023" class="csl-entry">
<p>Hatamizadeh, A., Yin, H., Molchanov, P., Myronenko, A., Li, W., Dogra,
P., Feng, A., Flores, M. G., Kautz, J., Xu, D., &amp; Roth, H. R. (2023). Do
Gradient Inversion Attacks Make Federated Learning Unsafe? <em>IEEE
Transactions on Medical Imaging</em>, 1–1.
<a href="https://doi.org/10.1109/TMI.2023.3239391">https://doi.org/10.1109/TMI.2023.3239391</a></p>
</div>
<div id="ref-ioffeBatchNormalizationAccelerating2015" class="csl-entry">
<p>Ioffe, S., &amp; Szegedy, C. (2015). <em>Batch Normalization: Accelerating Deep
Network Training by Reducing Internal Covariate Shift</em> (No.
arXiv:1502.03167). arXiv. <a href="https://doi.org/10.48550/arXiv.1502.03167">https://doi.org/10.48550/arXiv.1502.03167</a></p>
</div>
<div id="ref-kaissisEndtoendPrivacyPreserving2021" class="csl-entry">
<p>Kaissis, G., Ziller, A., Passerat-Palmbach, J., Ryffel, T., Usynin, D.,
Trask, A., Lima, I., Mancuso, J., Jungmann, F., Steinborn, M.-M., Saleh,
A., Makowski, M., Rueckert, D., &amp; Braren, R. (2021). End-to-end privacy
preserving deep learning on multi-institutional medical imaging. <em>Nature
Machine Intelligence</em>, <em>3</em>(6), 473–484.
<a href="https://doi.org/10.1038/s42256-021-00337-8">https://doi.org/10.1038/s42256-021-00337-8</a></p>
</div>
<div id="ref-konecnyFederatedLearningStrategies2017" class="csl-entry">
<p>Konečný, J., McMahan, H. B., Yu, F. X., Richtárik, P., Suresh, A. T., &amp;
Bacon, D. (2017). <em>Federated Learning: Strategies for Improving
Communication Efficiency</em> (No. arXiv:1610.05492). arXiv.
<a href="https://doi.org/10.48550/arXiv.1610.05492">https://doi.org/10.48550/arXiv.1610.05492</a></p>
</div>
<div id="ref-kononenkoMachineLearningMedical2001" class="csl-entry">
<p>Kononenko, I. (2001). Machine learning for medical diagnosis: History,
state of the art and perspective. <em>Artificial Intelligence in Medicine</em>,
<em>23</em>(1), 89–109. <a href="https://doi.org/10.1016/S0933-3657(01)00077-X">https://doi.org/10.1016/S0933-3657(01)00077-X</a></p>
</div>
<div id="ref-leePrivacyPreservingMachineLearning2022" class="csl-entry">
<p>Lee, J.-W., Kang, H., Lee, Y., Choi, W., Eom, J., Deryabin, M., Lee, E.,
Lee, J., Yoo, D., Kim, Y.-S., &amp; No, J.-S. (2022). Privacy-Preserving
Machine Learning With Fully Homomorphic Encryption for Deep Neural
Network. <em>IEEE Access</em>, <em>10</em>, 30039–30054.
<a href="https://doi.org/10.1109/ACCESS.2022.3159694">https://doi.org/10.1109/ACCESS.2022.3159694</a></p>
</div>
<div id="ref-liResSFLResistanceTransfer2022" class="csl-entry">
<p>Li, J., Rakin, A. S., Chen, X., He, Z., Fan, D., &amp; Chakrabarti, C.
(2022). ResSFL: A Resistance Transfer Framework for Defending Model
Inversion Attack in Split Federated Learning. <em>Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>,
10194–10202.</p>
</div>
<div id="ref-liFedBNFederatedLearning2021" class="csl-entry">
<p>Li, X., Jiang, M., Zhang, X., Kamp, M., &amp; Dou, Q. (2021). <em>FedBN:
Federated Learning on Non-IID Features via Local Batch Normalization</em>
(No. arXiv:2102.07623). arXiv.
<a href="https://doi.org/10.48550/arXiv.2102.07623">https://doi.org/10.48550/arXiv.2102.07623</a></p>
</div>
<div id="ref-liPrivacyThreatsAnalysis2021" class="csl-entry">
<p>Li, Y., Bao, Y., Xiang, L., Liu, J., Chen, C., Wang, L., &amp; Wang, X.
(2021). <em>Privacy Threats Analysis to Secure Federated Learning</em> (No.
arXiv:2106.13076). arXiv. <a href="https://arxiv.org/abs/2106.13076">https://arxiv.org/abs/2106.13076</a></p>
</div>
<div id="ref-liuSummaryChatGPTGPT42023" class="csl-entry">
<p>Liu, Y., Han, T., Ma, S., Zhang, J., Yang, Y., Tian, J., He, H., Li, A.,
He, M., Liu, Z., Wu, Z., Zhu, D., Li, X., Qiang, N., Shen, D., Liu, T.,
&amp; Ge, B. (2023). Summary of ChatGPT/GPT-4 Research and Perspective
Towards the Future of Large Language Models. In <em>arXiv.org</em>.
https://arxiv.org/abs/2304.01852v3.</p>
</div>
<div id="ref-mcmahanFederatedLearningCollaborative2017"
class="csl-entry">
<p>McMahan, B., &amp; Ramage, D. (2017). <em>Federated Learning: Collaborative
Machine Learning without Centralized Training Data</em>.
https://ai.googleblog.com/2017/04/federated-learning-collaborative.html.</p>
</div>
<div id="ref-nasrComprehensivePrivacyAnalysis2019" class="csl-entry">
<p>Nasr, M., Shokri, R., &amp; Houmansadr, A. (2019). Comprehensive Privacy
Analysis of Deep Learning: Passive and <span class="nocase">Active
White-box Inference Attacks</span> against Centralized and Federated
Learning. <em>2019 IEEE Symposium on Security and Privacy (SP)</em>, 739–753.
<a href="https://doi.org/10.1109/SP.2019.00065">https://doi.org/10.1109/SP.2019.00065</a></p>
</div>
<div id="ref-nguyenActiveMembershipInference2023" class="csl-entry">
<p>Nguyen, T., Lai, P., Tran, K., Phan, N., &amp; Thai, M. T. (2023). <em>Active
Membership Inference Attack under Local Differential Privacy in
Federated Learning</em> (No. arXiv:2302.12685). arXiv.
<a href="https://doi.org/10.48550/arXiv.2302.12685">https://doi.org/10.48550/arXiv.2302.12685</a></p>
</div>
<div id="ref-opreaAdversarialMachineLearning2023" class="csl-entry">
<p>Oprea, A., &amp; Vassilev, A. (2023). <em>Adversarial Machine Learning: A
Taxonomy and Terminology of Attacks and Mitigations (Draft)</em> (NIST AI
100-2e2023 ipd). National Institute of Standards and Technology.</p>
</div>
<div id="ref-riekeFutureDigitalHealth2020" class="csl-entry">
<p>Rieke, N., Hancox, J., Li, W., Milletarì, F., Roth, H. R., Albarqouni,
S., Bakas, S., Galtier, M. N., Landman, B. A., Maier-Hein, K., Ourselin,
S., Sheller, M., Summers, R. M., Trask, A., Xu, D., Baust, M., &amp;
Cardoso, M. J. (2020). The future of digital health with federated
learning. <em>NPJ Digital Medicine</em>, <em>3</em>, 119.
<a href="https://doi.org/10.1038/s41746-020-00323-1">https://doi.org/10.1038/s41746-020-00323-1</a></p>
</div>
<div id="ref-shinEmpiricalAnalysisImage2023" class="csl-entry">
<p>Shin, S., Boyapati, M., Suo, K., Kang, K., &amp; Son, J. (2023). An
empirical analysis of image augmentation against model inversion attack
in federated learning. <em>Cluster Computing</em>, <em>26</em>(1), 349–366.
<a href="https://doi.org/10.1007/s10586-022-03596-1">https://doi.org/10.1007/s10586-022-03596-1</a></p>
</div>
<div id="ref-shortenSurveyImageData2019" class="csl-entry">
<p>Shorten, C., &amp; Khoshgoftaar, T. M. (2019). A survey on Image Data
Augmentation for Deep Learning. <em>Journal of Big Data</em>, <em>6</em>(1), 60.
<a href="https://doi.org/10.1186/s40537-019-0197-0">https://doi.org/10.1186/s40537-019-0197-0</a></p>
</div>
<div id="ref-suriSubjectMembershipInference2022" class="csl-entry">
<p>Suri, A., Kanani, P., Marathe, V. J., &amp; Peterson, D. W. (2022). Subject
Membership Inference Attacks in Federated Learning. In <em>arXiv.org</em>.
https://arxiv.org/abs/2206.03317v3.</p>
</div>
<div id="ref-tolpeginDataPoisoningAttacks2020" class="csl-entry">
<p>Tolpegin, V., Truex, S., Gursoy, M. E., &amp; Liu, L. (2020). Data Poisoning
Attacks Against Federated Learning Systems. In <em>arXiv.org</em>.
https://arxiv.org/abs/2007.08432v2.</p>
</div>
</div>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p>More types of machine learning exist (Oprea &amp; Vassilev, 2023), but
the details of their taxonomy are not important for this discussion.
The focus will be on Federated Learning which, almost exclusively
uses supervised learning. <a href="#fnref1" class="no-underline">¶</a></p>
</li>
<li id="fn2" class="footnote-item"><p>Other network topologies are possible within Federated Learning.
One notable example is a peer-to-peer network which allows for a
completely decentralized machine-learning approach. Most actual
implemented systems, however, have a normal client-server structure
(Abad et al., 2022). <a href="#fnref2" class="no-underline">¶</a></p>
</li>
<li id="fn3" class="footnote-item"><p>Often, novel attack proposals also include possible
countermeasures. Some of the papers covered in the last section,
therefore, have also been included in this section. <a href="#fnref3" class="no-underline">¶</a></p>
</li>
<li id="fn4" class="footnote-item"><p>Often, novel attack proposals also include possible
countermeasures. Some of the papers covered in the last section,
therefore, have also been included in this section. <a href="#fnref4" class="no-underline">¶</a></p>
</li>
<li id="fn5" class="footnote-item"><p>Often, novel attack proposals also include possible
countermeasures. Some of the papers covered in the last section,
therefore, have also been included in this section. <a href="#fnref5" class="no-underline">¶</a></p>
</li>
</ol>
</section>
</article></div></main><footer class="mt-8 p-4 px-8"><div class="container max-w-3xl mx-auto text-neutral-500 dark:text-neutral-400 flex justify-between"><div>© 2022 - Zohar Cochavi</div><a href="https://github.com/cochaviz/bunkernet">check the source</a></div></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"frontmatter":{"abstract":"Federated Learning (FL) is a privacy-preserving approach to distributed machine learning, but it is vulnerable to inference attacks. Inference attacks aim to extract information about regarding model or data used in FL, directly threatening one of its core principles. This essay provides an overview of state-of-the-art inference attacks in FL and their implications for data privacy. It introduces the basics of FL, different types of inference attacks (model inversion, membership inference, property inference), and provides an overview of recent research on gradient inversion and membership inference attacks. We emphasize the need for robust defenses to safeguard sensitive information in FL systems, along with the importance of future research in addressing these increasingly realistic threats.","author":"Zohar Cochavi","autoEqnLabels":false,"autoSectionLabels":false,"bibliography":["src/bibliography.bib"],"ccsDelim":", ","ccsLabelSep":"—","ccsTemplate":"$$i$$$$ccsLabelSep$$$$t$$","chapDelim":".","chapters":false,"chaptersDepth":1,"classoption":["compsoc"],"codeBlockCaptions":false,"cref":false,"crossrefYaml":"pandoc-crossref.yaml","csl":"src/apa.csl","documentclass":"IEEEtran","eqLabels":"arabic","eqnBlockInlineMath":false,"eqnBlockTemplate":"\u003ctable\u003e\n\u003ccolgroup\u003e\n\u003ccol style=\"width: 90%\" /\u003e\n\u003ccol style=\"width: 10%\" /\u003e\n\u003c/colgroup\u003e\n\u003ctbody\u003e\n\u003ctr class=\"odd\"\u003e\n\u003ctd style=\"text-align: center;\"\u003e\u003cspan\nclass=\"math display\"\u003e\u003cem\u003et\u003c/em\u003e\u003c/span\u003e\u003c/td\u003e\n\u003ctd style=\"text-align: right;\"\u003e\u003cspan\nclass=\"math display\"\u003e\u003cem\u003ei\u003c/em\u003e\u003c/span\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n","eqnIndexTemplate":"($$i$$)","eqnInlineTemplate":"$$e$$$$equationNumberTeX$${$$i$$}","eqnPrefix":["eq.","eqns."],"eqnPrefixTemplate":"$$p$$ $$i$$","equationNumberTeX":"\\qquad","figLabels":"arabic","figPrefix":["fig.","figs."],"figPrefixTemplate":"$$p$$ $$i$$","figureTemplate":"$$figureTitle$$ $$i$$$$titleDelim$$ $$t$$","figureTitle":"Figure","lastDelim":", ","linkReferences":false,"listings":false,"listingTemplate":"$$listingTitle$$ $$i$$$$titleDelim$$ $$t$$","listingTitle":"Listing","listItemTitleDelim":".","lofItemTemplate":"$$lofItemTitle$$$$i$$$$listItemTitleDelim$$ $$t$$  \n","lofTitle":"# List of Figures\n","lolItemTemplate":"$$lolItemTitle$$$$i$$$$listItemTitleDelim$$ $$t$$  \n","lolTitle":"# List of Listings\n","lotItemTemplate":"$$lotItemTitle$$$$i$$$$listItemTitleDelim$$ $$t$$  \n","lotTitle":"# List of Tables\n","lstLabels":"arabic","lstPrefix":["lst.","lsts."],"lstPrefixTemplate":"$$p$$ $$i$$","nameInLink":false,"numberSections":false,"pairDelim":", ","rangeDelim":"-","refDelim":", ","refIndexTemplate":"$$i$$$$suf$$","secHeaderDelim":null,"secHeaderTemplate":"$$i$$$$secHeaderDelim[n]$$$$t$$","secLabels":"arabic","secPrefix":["sec.","secs."],"secPrefixTemplate":"$$p$$ $$i$$","sectionsDepth":0,"subfigGrid":false,"subfigLabels":"alpha a","subfigureChildTemplate":"$$i$$","subfigureRefIndexTemplate":"$$i$$$$suf$$ ($$s$$)","subfigureTemplate":"$$figureTitle$$ $$i$$$$titleDelim$$ $$t$$. $$ccs$$","tableEqns":false,"tableTemplate":"$$tableTitle$$ $$i$$$$titleDelim$$ $$t$$","tableTitle":"Table","tblLabels":"arabic","tblPrefix":["tbl.","tbls."],"tblPrefixTemplate":"$$p$$ $$i$$","title":"Inference Attacks on Federated Learning - A Survey","titleDelim":":","date":"2023-06-26","tags":["survey","adversarial machine learning","federated learning"]},"content":"\n## Introduction\n\nMachine learning models have demonstrated remarkable capabilities in\ninterpreting and deriving insights from data, leading to significant\nadvancements in fields such as medical diagnostics (Kononenko, 2001) and\nnatural language processing (Liu et al., 2023). However, these\napplications often involve handling privacy-sensitive data (Rieke et\nal., 2020), which can be inferred from trained models (Fredrikson et\nal., 2015), raising concerns about data privacy and security. Federated\nLearning is a privacy-preserving approach to distributed machine\nlearning that aims to address these concerns (McMahan \u0026 Ramage, 2017)\nbut is not immune to potential exploits (Abad et al., 2022). Inference\nAttacks directly threaten this privacy-preserving feature by attempting\nto extract information about the model and the data it was trained on\n(Geiping et al., 2020).\n\nIn this essay, I provide an overview of state-of-the-art Inversion\nAttacks and their defenses to Federated Learning. Besides informing on\nthe state-of-the-art, this essay should provide an introduction to the\nsubject for both machine learning and cyber security specialists. First,\nwe will discuss the basics of Federated Learning, introduce attacks on\nMachine Learning models, and consider a taxonomy of Inference Attacks on\nFederated Learning. Then, I will present novel attacks and some of their\ndefenses. Finally, we will discuss the threat these attacks present when\nconsidering the defenses and present future work to accommodate for\nthese threats.\n\n## Background\n\nIn this section, the necessary background information will be\nintroduced. The background is considered whatever already existed until\nlast year (March 2022). We will provide a concise overview of machine\nlearning principles, to then discuss the workings of Federated Learning\n(FL). Having covered the necessary machine learning knowledge, the\ndiscussion will move to how one would attack such systems. Finally, we\nfocus on previous inference attacks as summarized and discussed by (Abad\net al., 2022).\n\n### Machine Learning\n\nThe goal of any machine learning algorithm is to predict some label or\nvalue given familiar but unseen data. For the purposes of this\ndiscussion, the machine-learning process can be separated into 3 stages:\n\n1.  Training\n2.  Testing/Evaluation\n3.  Deployment\n\nDuring training, the machine learning model, $f$ is given a set of\ntuples $\\{(x_i, y_i)\\}$. The learning algorithm then adjusts the model\nparameters, $\\theta$, such that the model after training, $f_\\theta$,\nmaps the input features $x$ to the target value(s) $y$. Depending on the\nlearning task, $y$ could be a continuous value (regression), a binary\nvalue (binary classification), or a set of discrete values (multi-class\nclassification)[^1] (Abad et al., 2022; Chakraborty et al., 2018).\n\nThe testing phase assesses the performance of the model. We take a\nsimilar, but unseen set of tuples $\\{(x, y)\\}$ and test whether the\nmodel, $f_\\theta(x)$, returns the correct value(s). It’s important to\nonly test on *unseen* data since the aim is to assess the *generalizing*\nability of the model. One can imagine the performance to be higher if\nthe same data that was used to train, was used to test.\n\nAfter the model is trained and evaluated, it is then deployed. Often\naccessed via a public or private API. In the case of Federated Learning,\nthis process is iterative.\n\n### Federated Learning\n\nFederated Learning (FL) is a method of delegating, or democratizing, the\ntraining stage of a machine learning algorithm. Its benefits are\nthreefold (Konečný et al., 2017):\n\n1.  It avoids sharing *raw* personal data with a third party.\n2.  Processing resources are delegated.\n3.  Data that is fragmented over multiple locations can still be used\n    for training.\n\nThe process, generally, works as follows. Each client in set of clients,\n$C=\\{c_0, \\ldots, c_n\\}$, trains a private machine learning model their\nrespective dataset. Information about this trained model is then sent to\na central server that *aggregates* the information from all clients into\na single model. The newly trained model is then sent back to the clients\nfor another iteration[^2] Konečný et al. (2017).\n\n\u003cfigure\u003e\n\u003cimg src=\"/images/post/inference-attacks-on-federated-learning/client-server-fl.svg\"\nalt=\"Typical Federated Learning network topology. The client, c_i, sends the gradient, \\nabla Q(\\theta_i), and/or weights, \\theta_i, of a particular iteration i. The central server then sends the updated model parameters \\theta_{i+1} initiating the next iteration.\" /\u003e\n\u003cfigcaption aria-hidden=\"true\"\u003eTypical Federated Learning network\ntopology. The client, \u003cspan\nclass=\"math inline\"\u003e\u003cem\u003ec\u003c/em\u003e\u003csub\u003e\u003cem\u003ei\u003c/em\u003e\u003c/sub\u003e\u003c/span\u003e, sends the\ngradient, \u003cspan\nclass=\"math inline\"\u003e∇\u003cem\u003eQ\u003c/em\u003e(\u003cem\u003eθ\u003c/em\u003e\u003csub\u003e\u003cem\u003ei\u003c/em\u003e\u003c/sub\u003e)\u003c/span\u003e,\nand/or weights, \u003cspan\nclass=\"math inline\"\u003e\u003cem\u003eθ\u003c/em\u003e\u003csub\u003e\u003cem\u003ei\u003c/em\u003e\u003c/sub\u003e\u003c/span\u003e, of a\nparticular iteration \u003cspan class=\"math inline\"\u003e\u003cem\u003ei\u003c/em\u003e\u003c/span\u003e. The\ncentral server then sends the updated model parameters \u003cspan\nclass=\"math inline\"\u003e\u003cem\u003eθ\u003c/em\u003e\u003csub\u003e\u003cem\u003ei\u003c/em\u003e + 1\u003c/sub\u003e\u003c/span\u003e\ninitiating the next iteration.\u003c/figcaption\u003e\n\u003c/figure\u003e\n\nVarious aggregation algorithms exist. The most popular of which are\n*Federated Stochastic Gradient Descent* (FedSGD) and *Federated\nAveraging* (FedAvg). These use the *gradient* (the function that\ndescribes how to optimize the model) or the aforementioned parameters,\n$\\theta$, of the client models respectively when communicating model\nupdates Gu et al. (2022). While technical details of these algorithms\nare not crucial to this discussion, it is important to note that both\nthe gradient and the weights contain embedded information about the\nclient’s dataset (Fredrikson et al., 2015; Geiping et al., 2020).\n\n#### Vertical versus Horizontal FL\n\nLastly, there are two types of FL: Horizontal Federated Learning (HFL),\nor cross-device Federated Learning; and Vertical Federated Learning\n(VFL), or cross-silo Federated Learning (Abad et al., 2022; Suri et al.,\n2022). These differentiate in the way the client data is aligned to\nprovide a trainable model.\n\nIn the former, devices all collect data on the same features, but their\nsample space is not equal (the distributions might not align, and the\nsize can be different). This is the most used type of Federated\nLearning, which also is how companies such as Google train their models\non user data (McMahan \u0026 Ramage, 2017).\n\nIn the latter, we can imagine a set of hospitals or companies (or *data\nsilos*) that need to train a model on *all* the available user data. The\ndata, however, cannot be directly shared and the kind of data they\ncollect on their users is also different. Regardless, each user, or\n*subject*, is present in each database. They thus share the same sample\nspace but differ in the features they train their local models on.\n\n### Attacking Machine Learning\n\nThe machine learning approaches discussed so far (normal/*centralized*\nlearning and Federated Learning) contain several points at which an\nattacker could intervene to exploit various characteristics of the\nsystem. Before discussing inference attacks that take place in later\nstages of the machine learning pipeline, let us briefly discuss other\npotential threats to machine learning models.\n\nThe phases discussed in the first section (training, testing, and\ndeployment) correspond directly to different attacks which can be\ncategorized as follows (Chakraborty et al., 2018).\n\n1.  *Poisoning Attack*: This type of attack, known as contamination of\n    the training data, takes place during the training time of the\n    machine learning model. An adversary tries to poison the training\n    data by injecting carefully designed samples to compromise the whole\n    learning process eventually.\n\n2.  *Evasion Attack*: This is the most common type of attack in the\n    adversarial setting. The adversary tries to evade the system by\n    adjusting malicious samples during the testing phase. This setting\n    does not assume any influence over the training data.\n\n3.  *Inference/Exploratory Attack*: These attacks do not influence any\n    dataset. Given black-box access to the model, they try to gain as\n    much knowledge as possible about the learning algorithm of the\n    underlying system and pattern in training data.\n\nWhile the first two also pose potential threats to the FL scheme and are\nvery popular in centralized machine learning, they are considerably\nharder to perform on Federated Learning as the data is distributed\n(Tolpegin et al., 2020). Databases of multiple clients have to be\ncompromised to create an exploit that is comparable to that of a\ncentralized machine-learning approach.\n\nInference attacks, however, threaten the privacy guarantees FL attempts\nto give. Inference attacks specifically try to *infer* information about\nthe dataset the model was trained on or the model itself. Thereby\nthreatening the confidentiality of the database, and thus the privacy,\nof the victims involved (Abad et al., 2022). Since they actively infer\ninformation about a deployed system, the amount of information on the\nsystem determines how powerful such an attack could be. For this reason,\nthey are also further specified as *white-box* or *black-box*, and\nsometimes *grey-box* inference attacks (Nasr et al., 2019).\n\n### Inference Attacks\n\nInference attacks can be applied to both centralized machine learning\nmodels and Federated Learning schemes. Many of the principles we will\ncover apply to both centralized and Federated Learning, but the focus\nwill be on applications on FL. Specifically, we will provide an overview\nof attack classifications as given by Abad et al. (2022).\n\nFirstly, depending on the target information the attacker attempts to\ninfer, the attack is classified as follows:\n\n- *Model Inversion*: In model inversion, the attacker attempts to invert\n  the machine learning model. Thereby finding the data point\n  corresponding to a certain label. Fredrikson et al. (2015) were able\n  to invert a facial recognition model, allowing them to recover the\n  image of an individual known to be in the training dataset.\n\n- *Membership Inference*: In this attack, the goal is to determine\n  whether a data point $(x, y)$ was part of the training set. In FL it\n  is also possible to determine whether a data point was part of the\n  training set of a particular client.\n\n- *Property Inference*: Property inference attempts to detect whether\n  the dataset used to train the (local) model contains some property\n  (e.g. whether images are noisy) (Ganju et al., 2018).\n\nSecondly, when considering a malicious central server, the attack can be\nclassified according to the manner in which the attacker interferes with\nthe training procedure (Abad et al., 2022):\n\n- *Passive*: Also known as an *honest-but-curious* scenario. The\n  attacker can only read or eavesdrop on communications (i.e. the\n  weights and gradient transmitted between the clients and the server),\n  and the local model and dataset in case of an honest-but-curious\n  client.\n\n- *Active*: The attack is more effective than the former but less\n  stealthy. It essentially changes the learning goal from minimizing\n  loss to maximizing inferred information from the victim.\n\nLastly, attacks can be categorized based on the position of the attacker\nin the network:\n\n- *Local*: The attacker is a client, i.e. they can only access *their*\n  database, parameters, and the global parameters they receive from the\n  server.\n\n- *Global*: The attacker is the central server. They do not have access\n  to any databases but can access the gradients/parameters sent by all\n  clients and the global model.\n\n## Inference Attacks in Federated Learning\n\nThis section will discuss the state-of-the-art of inference attacks on\nFederated Learning. Specifically, we will discuss progress in the field\nas of March 2022. The research presented here was found primarily by\nquerying Google Scholar with the terms “Inference Attacks on Federated\nLearning”, “Membership Inference on Federated Learning”, “Model\nInversion on Federated Learning”, and “Gradient Inversion on Federated\nLearning”. The next section will cover the threats these advances pose\nto current systems.\n\nFirst, we will discuss various attacks, focusing not only on their\nperformance but paying special attention to the scenario in which the\nresearchers placed the hypothetical adversary. Then, we will cover\ndefenses to some of these attacks.\n\n### Attacking\n\nVarious types of attacks fall under the umbrella of inference attacks.\nAs of the writing of this essay, the most popular are *Membership\nInference* and *Gradient Inversion* as these show the most results.\n*Passive* inference attacks are more often covered than their *active*\ncounterparts. For each paper, we annotate the type of attack (see\n[Inference Attacks](#inference-attacks)), summarize the findings of the\nauthors, and briefly discuss them.\n\n#### Do Gradient Inversion Attacks Make Federated Learning Unsafe?\n\nKeywords: *Model Inversion*, *Local/Global*, *Cross-Device/HFL*,\n*Passive*\n\nHatamizadeh et al. (2023) performed image reconstruction using gradient\ninversion while relaxing a strong assumption made in prior work\nregarding Batch Normalization (BN) (Ioffe \u0026 Szegedy, 2015). BN is a\ntechnique used in neural networks that significantly improve the\nlearning rate and stability, and is therefore ubiquitous in modern\nmachine learning. The technique introduces two learned parameters,\n$\\beta$ and $\\gamma$, which thus change during the learning\nprocess(Ioffe \u0026 Szegedy, 2015). Previous work has assumed these\nstatistics to be static (Geiping et al., 2020; Kaissis et al., 2021),\nintroducing an error that would compound over time. The authors were\nable to reliably reconstruct images without assuming static BN\nstatistics. The authors make a strong case for an inversion attack that\ncould be used in practice but still rely on priors (approximations of\nthe image) to make accurate reconstructions.\n\n#### Improved Gradient Inversion Attacks and Defenses in Federated Learning\n\nKeywords: *Membership Inference*, *Local/Global*, *Cross-Device/HFL*,\n*Passive*, *White-Box*\n\nGeng et al. (2023) proposed a framework for inverting both\n*FedAVG*-based and *FedSGD*-based networks in an “honest-but-curious”\nscenario. They mention prior work has failed to effectively perform\ngradient inversion when FL uses the *FedAVG* aggregation algorithm.\nFurthermore, they specify methods for fine-tuning the performance of\nimage restoration in the inverted model, allowing them to restore images\nthat were introduced 16 epochs before the current iteration. As\nFederated Learning is an iterative process, one can imagine that the\nfurther a data point is removed from the current iteration, the harder\nit is to infer from the current gradient. While their results are\npromising, they do assume a white-box attack scenario making their\nattack harder to perform.\n\n#### CS-MIA: Membership Inference Attack Based on Prediction Confidence Series in Federated Learning\n\nKeywords: *Membership Inference*, *Local/Global*, *Cross-Device/VFL*,\n*Passive*\n\nGu et al. (2022) were able to determine whether data points are members\nof certain datasets by following the trend in their classification\nconfidence. Over time, the global model should perform less well on\nparticipants’ private data, meaning that member data should follow a\ndifferent trend compared to non-member data. They then train a\nsupervised model the determine whether data points were part of the\ntraining set based on this assumption. The model is then used to\ndetermine the probability of unseen data being part of the target\ntraining data set. They show high accuracy and F1-scores for all\ndatasets with the lowest performer being MNIST (around 60% compared to\n\\\u003e90% for the other datasets). Still, the proposed solution scores the\nbest out of all included approaches by a significant margin.\n\n#### Subject Membership Inference Attacks in Federated Learning\n\nKeywords: *Membership Inference*, *Local/Global*, *Cross-Silo/VFL*,\n*Passive*\n\nIn a black-box setting, Suri et al. (2022) propose a method for what\nthey call *Subject Inference* (see [Federated\nLearning](#federated-learning)). They describe previous work as being\ndisconnected from real-world scenarios as it (i) includes information\nadversaries would not normally have access to and (ii) assumes the\nadversary is looking for data points rather than individuals. Instead of\ndetermining whether one particular data point was part of the training\nset, the authors attempt to infer whether an individual, a *subject*,\n(or rather their distribution) is present in the dataset given some\npreexisting information on them. They show the attack to be very\neffective in various real-world datasets while also increasing the\nrealism of the scenario. They show Subject Inference to be a real threat\nto user privacy.\n\n#### Active Membership Inference Attack under Local Differential Privacy in Federated Learning.\n\nKeywords: *Membership Inference*, *Local/Global*, *Cross-Device/HFL*,\n*Active*\n\nDifferent from other works, Nguyen et al. (2023) considers a maximally\nmalicious, i.e. *active*, membership inference attack. They implement a\nmethod for inferring membership of a particular data point in the\npresence of differential privacy (Dwork \u0026 Roth, 2013). Differential\nprivacy obscures the relation of the individual to the data point,\nwithout affecting the patterns used for training machine learning\nmodels. The authors show that their method performs well, even under\nsuch obscuring of the data. Furthermore, the attack only starts to\ndegrade after the level of obscurity interferes with model performance.\nThey show that more rigorous privacy methods should be proposed to deal\nwith such attacks.\n\n### Defending\n\nTo combat inference attacks, we discuss potential defenses against them.\nSome of the papers that are included have been discussed in the last\nsection. These have been marked with a footnote accordingly [^3]. For\neach paper, we will summarize the proposed measures and briefly discuss\nthem.\n\n#### Improved Gradient Inversion Attacks and Defenses in Federated Learning[^4]\n\nGeng et al. (2023) found that labels that only appeared only once were\nmore prone to their proposed inversion attacks (see\n[Attacks](#improved-gradient-inversion-attacks-and-defenses-in-federated-learning)).\nThey also mention the use of larger batch sizes in the global model\n(i.e. more clients) to reduce the amount of private information embedded\nin a single batch. Lastly, they claim FedAVG possesses “stronger privacy\npreserving capabilities than FedSGD”. As this was included in the\ndiscussion of their attack-oriented paper, they do not evaluate these\nclaims further.\n\n#### Do Gradient Inversion Attacks Make Federated Learning Unsafe?[^5]\n\nHatamizadeh et al. (2023) make several recommendations to make existing\nimplementations of FL safer, namely: (i) larger training sets, (ii)\nupdates from a larger number of iterations over different (iii) large\nbatch sizes. In addition, they mention three more changes that could\npotentially mitigate server-side (i.e. *Global*) gradient inversion\nattacks: (1) The use of *Homomorphic Encryption* (see [Future\nWork](#future-work)), (2) ensuring the attacker does not have knowledge\nof the model architecture, and (3) using an alternative aggregation\nalgorithm such as FedBN Andreux et al. (2020). The countermeasures\nprovided are relatively general. They also provided sources affirming\ntheir suspicions.\n\n#### An Empirical Analysis of Image Augmentation Against Model Inversion Attack in Federated Learning\n\nShin et al. (2023) propose the use of image augmentation as a more\nviable alternative to differential privacy (Dwork \u0026 Roth, 2013). Image\naugmentation is a data synthesis method that increases the size of the\ntraining set, and reduces over-fitting (Shorten \u0026 Khoshgoftaar, 2019).\nAs this introduces fake data while improving the overall performance of\nthe model, the authors suggest it could be used to mitigate model\ninversion attacks. The attack they used was introduced by (Geiping et\nal., 2020), and various more successful attacks have been constructed\nsince then Geng et al. (2023).\n\n#### ResSFL: A Resistance Transfer Framework for Defending Model Inversion Attack in Split Federated Learning\n\nIn a framework introduced by (J. Li et al., 2022), Split Federated\nLearning (SFL) (Annavaram \u0026 Avestimehr, n.d.) is augmented with a model\nthat attempts to invert the model before the client sends their model to\nthe central server. By choosing weights where the discriminator performs\npoorly, they claim to improve the resiliency of the scheme to model\ninversion attacks. They indeed show improvements over the standard\nimplementation of SFL but do not mention how this method compares to\nattacks on default FL.\n\n## Discussion\n\nIn this section, we will discuss the attacks and defenses as presented\nin the last section. Specifically, we determine the threats these\nattacks pose and if the defenses included could effectively mitigate\nthe. In the end, we propose new research directions that could help\nmitigate these threats.\n\n### Current Threats and Trends\n\nThe attacks presented show how Federated Learning might not be able to\nguarantee privacy. Privacy, thus, should still remain a concern even if\nFederated Learning brings stronger privacy guarantees than traditional\nmachine learning and its derivatives. Let us summarize the threats these\nattacks pose:\n\n- *More Realistic Scenarios*: Research starts to introduce more\n  realistic scenarios that could threaten current implementations of\n  Federated Learning. As the field matures, attacks seem to become more\n  realistic. Especially the work presented by Suri et al. (2022) poses a\n  real threat as it assumes a complete black-box attack with reasonable\n  assumptions while still showing good performance. Even in complete\n  black-box settings, however, we still assume the ability to intercept\n  and read the communications. Were this to be encrypted, such attacks\n  could possibly be mitigated (Y. Li et al., 2021).\n\n- *Increased Resilience Against Existing Privacy Measures*: Some of the\n  aforementioned papers has shown improvements concerning the evasion of\n  privacy-preserving measures. Nguyen et al. (2023) have shown how a\n  membership inference attack can be effectively performed in the\n  presence of differential privacy. Their method was effective to such a\n  degree that the attack was ineffective only once the privacy measures\n  started to affect model performance. The image augmentation\n  countermeasure proposed by (Shin et al., 2023) could be a viable\n  option. This countermeasure, however, was only tested in a *passive*\n  scenario.\n\n- *Stronger Attacks in Existing Scenarios*: As to be expected, some work\n  was focused on improving performance in existing scenarios. Gu et\n  al. (2022) have shown that much is still to be learned in the field by\n  proposing a relatively simple approach that improves upon all previous\n  methods by a large margin.\n\nSuch developments are not surprising, progress in both offense and\ndefense is to be expected. The speed at which research moves forward is\nvery impressive and suggests the field is still in the early stages of\ndevelopment. When considering using such new technologies in production,\nthis could be considered when assessing the security of such systems.\n\n### Future Work\n\nConsidering the aforementioned advances, the following directions could\nprovide useful for future research:\n\n1.  Consider using existing preprocessing methods for privacy\n    preservation. Shin et al. (2023) and Hatamizadeh et al. (2023) both\n    either use or suggest using existing pre-processing or other\n    learning-enhancing augmentations to improve privacy. Efforts toward\n    generalizing data *before* training might prove a solution to both\n    overfitting and privacy.\n\n2.  New attack methods would benefit from relaxing assumptions instead\n    of attempting to increase performance. By doing this, various of the\n    attacks shown have been to provide a more realistic view of the\n    privacy-preserving features of FL. While performance improvements\n    might provide interesting results and insights, focusing efforts on\n    exposing potential *realistic* threats would have a more direct\n    effect on our ability to assess FL from a privacy perspective.\n\n3.  Working, safe Homomorphic Encryption (HE) would hamper most of the\n    aforementioned attacks. Being able to encrypt data *before* training\n    a model would make inference attacks completely benign (Lee et al.,\n    2022). Work from the past year, however, was able to infer\n    privacy-sensitive information about the training set regardless of\n    the presence of HE (Y. Li et al., 2021). More research on the robust\n    use of HE could prove a catch-all solution for many of the presented\n    machine-learning attacks.\n\n## Conclusion\n\nThis essay has provided an overview for security specialists and machine\nlearning specialists to assess the current state of Inference Attacks in\nFederated Learning. Progress over the last year has shown the field to\nbe advancing quickly. Introducing successful attacks on new, more\nrealistic scenarios, showing the ability to circumvent mature\nprivacy-preserving, measures and improving the performance of existing\nmethods. The developments seen here are concerning given the prevalence\nof Federated Learning. More research is needed to assess how\nprivacy-preserving Federated Learning actually is and whether additional\ncountermeasures provide enough security to circumvent the apparent\nthreats presented here.\n\n## References\n\n\u003cdiv id=\"refs\" class=\"references csl-bib-body hanging-indent\"\nline-spacing=\"2\"\u003e\n\n\u003cdiv id=\"ref-abadSecurityPrivacyFederated2022\" class=\"csl-entry\"\u003e\n\nAbad, G., Picek, S., Ramírez-Durán, V. J., \u0026 Urbieta, A. (2022). *On the\nSecurity \u0026 Privacy in Federated Learning* (No. arXiv:2112.05423). arXiv.\n\u003chttps://arxiv.org/abs/2112.05423\u003e\n\n\u003c/div\u003e\n\n\u003cdiv id=\"ref-andreuxSiloedFederatedLearning2020\" class=\"csl-entry\"\u003e\n\nAndreux, M., Du Terrail, J. O., Beguier, C., \u0026 Tramel, E. W. (2020).\nSiloed Federated Learning for \u003cspan class=\"nocase\"\u003eMulti-centric\nHistopathology Datasets\u003c/span\u003e. In S. Albarqouni, S. Bakas, K.\nKamnitsas, M. J. Cardoso, B. Landman, W. Li, F. Milletari, N. Rieke, H.\nRoth, D. Xu, \u0026 Z. Xu (Eds.), *Domain Adaptation and Representation\nTransfer, and Distributed and Collaborative Learning* (Vol. 12444, pp.\n129–139). Springer International Publishing.\n\u003chttps://doi.org/10.1007/978-3-030-60548-3_13\u003e\n\n\u003c/div\u003e\n\n\u003cdiv id=\"ref-annavaramGroupKnowledgeTransfer\" class=\"csl-entry\"\u003e\n\nAnnavaram, C. H. M., \u0026 Avestimehr, S. (n.d.). *Group Knowledge Transfer:\nFederated Learning of Large CNNs at the Edge*.\n\n\u003c/div\u003e\n\n\u003cdiv id=\"ref-chakrabortyAdversarialAttacksDefences2018\"\nclass=\"csl-entry\"\u003e\n\nChakraborty, A., Alam, M., Dey, V., Chattopadhyay, A., \u0026 Mukhopadhyay,\nD. (2018). *Adversarial Attacks and Defences: A Survey* (No.\narXiv:1810.00069). arXiv. \u003chttps://arxiv.org/abs/1810.00069\u003e\n\n\u003c/div\u003e\n\n\u003cdiv id=\"ref-dworkAlgorithmicFoundationsDifferential2013\"\nclass=\"csl-entry\"\u003e\n\nDwork, C., \u0026 Roth, A. (2013). The Algorithmic Foundations of\nDifferential Privacy. *Foundations and Trends in Theoretical Computer\nScience*, *9*(3-4), 211–407. \u003chttps://doi.org/10.1561/0400000042\u003e\n\n\u003c/div\u003e\n\n\u003cdiv id=\"ref-fredriksonModelInversionAttacks2015\" class=\"csl-entry\"\u003e\n\nFredrikson, M., Jha, S., \u0026 Ristenpart, T. (2015). Model Inversion\nAttacks that Exploit Confidence Information and Basic Countermeasures.\n*Proceedings of the 22nd ACM SIGSAC Conference on Computer and\nCommunications Security*, 1322–1333.\n\u003chttps://doi.org/10.1145/2810103.2813677\u003e\n\n\u003c/div\u003e\n\n\u003cdiv id=\"ref-ganjuPropertyInferenceAttacks2018\" class=\"csl-entry\"\u003e\n\nGanju, K., Wang, Q., Yang, W., Gunter, C. A., \u0026 Borisov, N. (2018).\nProperty Inference Attacks on Fully Connected Neural Networks using\nPermutation Invariant Representations. *Proceedings of the 2018 ACM\nSIGSAC Conference on Computer and Communications Security*, 619–633.\n\u003chttps://doi.org/10.1145/3243734.3243834\u003e\n\n\u003c/div\u003e\n\n\u003cdiv id=\"ref-geipingInvertingGradientsHow2020\" class=\"csl-entry\"\u003e\n\nGeiping, J., Bauermeister, H., Dröge, H., \u0026 Moeller, M. (2020).\nInverting Gradients - How easy is it to break privacy in federated\nlearning? *Advances in Neural Information Processing Systems*, *33*,\n16937–16947.\n\n\u003c/div\u003e\n\n\u003cdiv id=\"ref-gengImprovedGradientInversion2023\" class=\"csl-entry\"\u003e\n\nGeng, J., Mou, Y., Li, Q., Li, F., Beyan, O., Decker, S., \u0026 Rong, C.\n(2023). Improved Gradient Inversion Attacks and Defenses in Federated\nLearning. *IEEE Transactions on Big Data*, 1–13.\n\u003chttps://doi.org/10.1109/TBDATA.2023.3239116\u003e\n\n\u003c/div\u003e\n\n\u003cdiv id=\"ref-guCSMIAMembershipInference2022\" class=\"csl-entry\"\u003e\n\nGu, Y., Bai, Y., \u0026 Xu, S. (2022). CS-MIA: Membership inference attack\nbased on prediction confidence series in federated learning. *Journal of\nInformation Security and Applications*, *67*, 103201.\n\u003chttps://doi.org/10.1016/j.jisa.2022.103201\u003e\n\n\u003c/div\u003e\n\n\u003cdiv id=\"ref-hatamizadehGradientInversionAttacks2023\" class=\"csl-entry\"\u003e\n\nHatamizadeh, A., Yin, H., Molchanov, P., Myronenko, A., Li, W., Dogra,\nP., Feng, A., Flores, M. G., Kautz, J., Xu, D., \u0026 Roth, H. R. (2023). Do\nGradient Inversion Attacks Make Federated Learning Unsafe? *IEEE\nTransactions on Medical Imaging*, 1–1.\n\u003chttps://doi.org/10.1109/TMI.2023.3239391\u003e\n\n\u003c/div\u003e\n\n\u003cdiv id=\"ref-ioffeBatchNormalizationAccelerating2015\" class=\"csl-entry\"\u003e\n\nIoffe, S., \u0026 Szegedy, C. (2015). *Batch Normalization: Accelerating Deep\nNetwork Training by Reducing Internal Covariate Shift* (No.\narXiv:1502.03167). arXiv. \u003chttps://doi.org/10.48550/arXiv.1502.03167\u003e\n\n\u003c/div\u003e\n\n\u003cdiv id=\"ref-kaissisEndtoendPrivacyPreserving2021\" class=\"csl-entry\"\u003e\n\nKaissis, G., Ziller, A., Passerat-Palmbach, J., Ryffel, T., Usynin, D.,\nTrask, A., Lima, I., Mancuso, J., Jungmann, F., Steinborn, M.-M., Saleh,\nA., Makowski, M., Rueckert, D., \u0026 Braren, R. (2021). End-to-end privacy\npreserving deep learning on multi-institutional medical imaging. *Nature\nMachine Intelligence*, *3*(6), 473–484.\n\u003chttps://doi.org/10.1038/s42256-021-00337-8\u003e\n\n\u003c/div\u003e\n\n\u003cdiv id=\"ref-konecnyFederatedLearningStrategies2017\" class=\"csl-entry\"\u003e\n\nKonečný, J., McMahan, H. B., Yu, F. X., Richtárik, P., Suresh, A. T., \u0026\nBacon, D. (2017). *Federated Learning: Strategies for Improving\nCommunication Efficiency* (No. arXiv:1610.05492). arXiv.\n\u003chttps://doi.org/10.48550/arXiv.1610.05492\u003e\n\n\u003c/div\u003e\n\n\u003cdiv id=\"ref-kononenkoMachineLearningMedical2001\" class=\"csl-entry\"\u003e\n\nKononenko, I. (2001). Machine learning for medical diagnosis: History,\nstate of the art and perspective. *Artificial Intelligence in Medicine*,\n*23*(1), 89–109. \u003chttps://doi.org/10.1016/S0933-3657(01)00077-X\u003e\n\n\u003c/div\u003e\n\n\u003cdiv id=\"ref-leePrivacyPreservingMachineLearning2022\" class=\"csl-entry\"\u003e\n\nLee, J.-W., Kang, H., Lee, Y., Choi, W., Eom, J., Deryabin, M., Lee, E.,\nLee, J., Yoo, D., Kim, Y.-S., \u0026 No, J.-S. (2022). Privacy-Preserving\nMachine Learning With Fully Homomorphic Encryption for Deep Neural\nNetwork. *IEEE Access*, *10*, 30039–30054.\n\u003chttps://doi.org/10.1109/ACCESS.2022.3159694\u003e\n\n\u003c/div\u003e\n\n\u003cdiv id=\"ref-liResSFLResistanceTransfer2022\" class=\"csl-entry\"\u003e\n\nLi, J., Rakin, A. S., Chen, X., He, Z., Fan, D., \u0026 Chakrabarti, C.\n(2022). ResSFL: A Resistance Transfer Framework for Defending Model\nInversion Attack in Split Federated Learning. *Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition*,\n10194–10202.\n\n\u003c/div\u003e\n\n\u003cdiv id=\"ref-liFedBNFederatedLearning2021\" class=\"csl-entry\"\u003e\n\nLi, X., Jiang, M., Zhang, X., Kamp, M., \u0026 Dou, Q. (2021). *FedBN:\nFederated Learning on Non-IID Features via Local Batch Normalization*\n(No. arXiv:2102.07623). arXiv.\n\u003chttps://doi.org/10.48550/arXiv.2102.07623\u003e\n\n\u003c/div\u003e\n\n\u003cdiv id=\"ref-liPrivacyThreatsAnalysis2021\" class=\"csl-entry\"\u003e\n\nLi, Y., Bao, Y., Xiang, L., Liu, J., Chen, C., Wang, L., \u0026 Wang, X.\n(2021). *Privacy Threats Analysis to Secure Federated Learning* (No.\narXiv:2106.13076). arXiv. \u003chttps://arxiv.org/abs/2106.13076\u003e\n\n\u003c/div\u003e\n\n\u003cdiv id=\"ref-liuSummaryChatGPTGPT42023\" class=\"csl-entry\"\u003e\n\nLiu, Y., Han, T., Ma, S., Zhang, J., Yang, Y., Tian, J., He, H., Li, A.,\nHe, M., Liu, Z., Wu, Z., Zhu, D., Li, X., Qiang, N., Shen, D., Liu, T.,\n\u0026 Ge, B. (2023). Summary of ChatGPT/GPT-4 Research and Perspective\nTowards the Future of Large Language Models. In *arXiv.org*.\nhttps://arxiv.org/abs/2304.01852v3.\n\n\u003c/div\u003e\n\n\u003cdiv id=\"ref-mcmahanFederatedLearningCollaborative2017\"\nclass=\"csl-entry\"\u003e\n\nMcMahan, B., \u0026 Ramage, D. (2017). *Federated Learning: Collaborative\nMachine Learning without Centralized Training Data*.\nhttps://ai.googleblog.com/2017/04/federated-learning-collaborative.html.\n\n\u003c/div\u003e\n\n\u003cdiv id=\"ref-nasrComprehensivePrivacyAnalysis2019\" class=\"csl-entry\"\u003e\n\nNasr, M., Shokri, R., \u0026 Houmansadr, A. (2019). Comprehensive Privacy\nAnalysis of Deep Learning: Passive and \u003cspan class=\"nocase\"\u003eActive\nWhite-box Inference Attacks\u003c/span\u003e against Centralized and Federated\nLearning. *2019 IEEE Symposium on Security and Privacy (SP)*, 739–753.\n\u003chttps://doi.org/10.1109/SP.2019.00065\u003e\n\n\u003c/div\u003e\n\n\u003cdiv id=\"ref-nguyenActiveMembershipInference2023\" class=\"csl-entry\"\u003e\n\nNguyen, T., Lai, P., Tran, K., Phan, N., \u0026 Thai, M. T. (2023). *Active\nMembership Inference Attack under Local Differential Privacy in\nFederated Learning* (No. arXiv:2302.12685). arXiv.\n\u003chttps://doi.org/10.48550/arXiv.2302.12685\u003e\n\n\u003c/div\u003e\n\n\u003cdiv id=\"ref-opreaAdversarialMachineLearning2023\" class=\"csl-entry\"\u003e\n\nOprea, A., \u0026 Vassilev, A. (2023). *Adversarial Machine Learning: A\nTaxonomy and Terminology of Attacks and Mitigations (Draft)* (NIST AI\n100-2e2023 ipd). National Institute of Standards and Technology.\n\n\u003c/div\u003e\n\n\u003cdiv id=\"ref-riekeFutureDigitalHealth2020\" class=\"csl-entry\"\u003e\n\nRieke, N., Hancox, J., Li, W., Milletarì, F., Roth, H. R., Albarqouni,\nS., Bakas, S., Galtier, M. N., Landman, B. A., Maier-Hein, K., Ourselin,\nS., Sheller, M., Summers, R. M., Trask, A., Xu, D., Baust, M., \u0026\nCardoso, M. J. (2020). The future of digital health with federated\nlearning. *NPJ Digital Medicine*, *3*, 119.\n\u003chttps://doi.org/10.1038/s41746-020-00323-1\u003e\n\n\u003c/div\u003e\n\n\u003cdiv id=\"ref-shinEmpiricalAnalysisImage2023\" class=\"csl-entry\"\u003e\n\nShin, S., Boyapati, M., Suo, K., Kang, K., \u0026 Son, J. (2023). An\nempirical analysis of image augmentation against model inversion attack\nin federated learning. *Cluster Computing*, *26*(1), 349–366.\n\u003chttps://doi.org/10.1007/s10586-022-03596-1\u003e\n\n\u003c/div\u003e\n\n\u003cdiv id=\"ref-shortenSurveyImageData2019\" class=\"csl-entry\"\u003e\n\nShorten, C., \u0026 Khoshgoftaar, T. M. (2019). A survey on Image Data\nAugmentation for Deep Learning. *Journal of Big Data*, *6*(1), 60.\n\u003chttps://doi.org/10.1186/s40537-019-0197-0\u003e\n\n\u003c/div\u003e\n\n\u003cdiv id=\"ref-suriSubjectMembershipInference2022\" class=\"csl-entry\"\u003e\n\nSuri, A., Kanani, P., Marathe, V. J., \u0026 Peterson, D. W. (2022). Subject\nMembership Inference Attacks in Federated Learning. In *arXiv.org*.\nhttps://arxiv.org/abs/2206.03317v3.\n\n\u003c/div\u003e\n\n\u003cdiv id=\"ref-tolpeginDataPoisoningAttacks2020\" class=\"csl-entry\"\u003e\n\nTolpegin, V., Truex, S., Gursoy, M. E., \u0026 Liu, L. (2020). Data Poisoning\nAttacks Against Federated Learning Systems. In *arXiv.org*.\nhttps://arxiv.org/abs/2007.08432v2.\n\n\u003c/div\u003e\n\n\u003c/div\u003e\n\n[^1]: More types of machine learning exist (Oprea \u0026 Vassilev, 2023), but\n    the details of their taxonomy are not important for this discussion.\n    The focus will be on Federated Learning which, almost exclusively\n    uses supervised learning.\n\n[^2]: Other network topologies are possible within Federated Learning.\n    One notable example is a peer-to-peer network which allows for a\n    completely decentralized machine-learning approach. Most actual\n    implemented systems, however, have a normal client-server structure\n    (Abad et al., 2022).\n\n[^3]: Often, novel attack proposals also include possible\n    countermeasures. Some of the papers covered in the last section,\n    therefore, have also been included in this section.\n\n[^4]: Often, novel attack proposals also include possible\n    countermeasures. Some of the papers covered in the last section,\n    therefore, have also been included in this section.\n\n[^5]: Often, novel attack proposals also include possible\n    countermeasures. Some of the papers covered in the last section,\n    therefore, have also been included in this section.\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"inference-attacks-on-federated-learning"},"buildId":"YZIEIcGJBIygEvu0QOl3a","isFallback":false,"gsp":true,"scriptLoader":[]}</script><script async="" src="//static.getclicky.com/101354370.js"></script><noscript><p><img alt="Clicky" width="1" height="1" src="//in.getclicky.com/101354370ns.gif"/></p></noscript></body></html>