{"pageProps":{"frontmatter":{"abstract":"Federated Learning (FL) is a privacy-preserving approach to distributed machine learning, but it is vulnerable to inference attacks. Inference attacks aim to extract information about regarding model or data used in FL, directly threatening one of its core principles. This essay provides an overview of state-of-the-art inference attacks in FL and their implications for data privacy. It introduces the basics of FL, different types of inference attacks (model inversion, membership inference, property inference), and provides an overview of recent research on gradient inversion and membership inference attacks. We emphasize the need for robust defenses to safeguard sensitive information in FL systems, along with the importance of future research in addressing these increasingly realistic threats.","author":"Zohar Cochavi","autoEqnLabels":false,"autoSectionLabels":false,"ccsDelim":",","ccsLabelSep":"—","ccsTemplate":"$$i$$$$ccsLabelSep$$$$t$$","chapDelim":".","chapters":false,"chaptersDepth":1,"classoption":["compsoc"],"codeBlockCaptions":true,"cref":false,"crossrefYaml":"pandoc-crossref.yaml","date":"2023-06-26","documentclass":"IEEEtran","eqLabels":"arabic","eqnBlockInlineMath":false,"eqnBlockTemplate":"<table>\n<colgroup>\n<col style=\"width: 90%\" />\n<col style=\"width: 10%\" />\n</colgroup>\n<tbody>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">\n<span class=\"math display\"><em>t</em></span>\n</td>\n<td style=\"text-align: right;\">\n<span class=\"math display\"><em>i</em></span>\n</td>\n</tr>\n</tbody>\n</table>\n","eqnIndexTemplate":"($$i$$)","eqnInlineTemplate":"$$e$$$$equationNumberTeX$${$$i$$}","eqnPrefix":["eq.","eqns."],"eqnPrefixTemplate":"$$p$$ $$i$$","figLabels":"arabic","figPrefix":["fig.","figs."],"figPrefixTemplate":"$$p$$ $$i$$","figureTemplate":"$$figureTitle$$ $$i$$$$titleDelim$$ $$t$$","figureTitle":"Figure","lastDelim":",","linkReferences":false,"listings":false,"listingTemplate":"$$listingTitle$$ $$i$$$$titleDelim$$ $$t$$","listingTitle":"Listing","listItemTitleDelim":".","lofItemTemplate":"$$lofItemTitle$$$$i$$$$listItemTitleDelim$$ $$t$$\n","lofTitle":"# List of Figures\n","lolItemTemplate":"$$lolItemTitle$$$$i$$$$listItemTitleDelim$$ $$t$$\n","lolTitle":"# List of Listings\n","lotItemTemplate":"$$lotItemTitle$$$$i$$$$listItemTitleDelim$$ $$t$$\n","lotTitle":"# List of Tables\n","lstLabels":"arabic","lstPrefix":["lst.","lsts."],"lstPrefixTemplate":"$$p$$ $$i$$","nameInLink":false,"numberSections":false,"pairDelim":",","rangeDelim":"-","refDelim":",","refIndexTemplate":"$$i$$$$suf$$","secHeaderTemplate":"$$i$$$$secHeaderDelim[n]$$$$t$$","secLabels":"arabic","secPrefix":["sec.","secs."],"secPrefixTemplate":"$$p$$ $$i$$","sectionsDepth":0,"subfigGrid":false,"subfigLabels":"alpha a","subfigureChildTemplate":"$$i$$","subfigureRefIndexTemplate":"$$i$$$$suf$$ ($$s$$)","subfigureTemplate":"$$figureTitle$$ $$i$$$$titleDelim$$ $$t$$. $$ccs$$","tableEqns":false,"tableTemplate":"$$tableTitle$$ $$i$$$$titleDelim$$ $$t$$","tableTitle":"Table","tags":["survey","adversarial machine learning","federated learning"],"tblLabels":"arabic","tblPrefix":["tbl.","tbls."],"tblPrefixTemplate":"$$p$$ $$i$$","title":"Inference Attacks on Federated Learning - A Survey","titleDelim":":"},"content":"\n## Introduction\n\nMachine learning models have demonstrated remarkable capabilities in\ninterpreting and deriving insights from data, leading to significant\nadvancements in fields such as medical diagnostics (Kononenko, 2001) and\nnatural language processing (Liu et al., 2023). However, these\napplications often involve handling privacy-sensitive data (Rieke et\nal., 2020), which can be inferred from trained models (Fredrikson et\nal., 2015), raising concerns about data privacy and security. Federated\nLearning is a privacy-preserving approach to distributed machine\nlearning that aims to address these concerns (McMahan & Ramage, 2017)\nbut is not immune to potential exploits (Abad et al., 2022). Inference\nAttacks directly threaten this privacy-preserving feature by attempting\nto extract information about the model and the data it was trained on\n(Geiping et al., 2020).\n\nIn this essay, I provide an overview of state-of-the-art Inversion\nAttacks and their defenses to Federated Learning. Besides informing on\nthe state-of-the-art, this essay should provide an introduction to the\nsubject for both machine learning and cyber security specialists. First,\nwe will discuss the basics of Federated Learning, introduce attacks on\nMachine Learning models, and consider a taxonomy of Inference Attacks on\nFederated Learning. Then, I will present novel attacks and some of their\ndefenses. Finally, we will discuss the threat these attacks present when\nconsidering the defenses and present future work to accommodate for\nthese threats.\n\n## Background\n\nIn this section, the necessary background information will be\nintroduced. The background is considered whatever already existed until\nlast year (March 2022). We will provide a concise overview of machine\nlearning principles, to then discuss the workings of Federated Learning\n(FL). Having covered the necessary machine learning knowledge, the\ndiscussion will move to how one would attack such systems. Finally, we\nfocus on previous inference attacks as summarized and discussed by (Abad\net al., 2022).\n\n### Machine Learning\n\nThe goal of any machine learning algorithm is to predict some label or\nvalue given familiar but unseen data. For the purposes of this\ndiscussion, the machine-learning process can be separated into 3 stages:\n\n1. Training\n2. Testing/Evaluation\n3. Deployment\n\nDuring training, the machine learning model, $f$ is given a set of\ntuples $\\{(x_i, y_i)\\}$. The learning algorithm then adjusts the model\nparameters, $\\theta$, such that the model after training, $f_\\theta$,\nmaps the input features $x$ to the target value(s) $y$. Depending on the\nlearning task, $y$ could be a continuous value (regression), a binary\nvalue (binary classification), or a set of discrete values (multi-class\nclassification)[^1] (Abad et al., 2022; Chakraborty et al., 2018).\n\nThe testing phase assesses the performance of the model. We take a\nsimilar, but unseen set of tuples $\\{(x, y)\\}$ and test whether the\nmodel, $f_\\theta(x)$, returns the correct value(s). It’s important to\nonly test on *unseen* data since the aim is to assess the *generalizing*\nability of the model. One can imagine the performance to be higher if\nthe same data that was used to train, was used to test.\n\nAfter the model is trained and evaluated, it is then deployed. Often\naccessed via a public or private API. In the case of Federated Learning,\nthis process is iterative.\n\n### Federated Learning\n\nFederated Learning (FL) is a method of delegating, or democratizing, the\ntraining stage of a machine learning algorithm. Its benefits are\nthreefold (Konečný et al., 2017):\n\n1. It avoids sharing *raw* personal data with a third party.\n2. Processing resources are delegated.\n3. Data that is fragmented over multiple locations can still be used\n    for training.\n\nThe process, generally, works as follows. Each client in set of clients,\n $C=\\{c_0, \\ldots, c_n\\}$, trains a private machine learning model their\nrespective dataset. Information about this trained model is then sent to\na central server that *aggregates* the information from all clients into\na single model. The newly trained model is then sent back to the clients\nfor another iteration[^2] Konečný et al. (2017).\n\n<figure>\n<img src=\"/images/post/inference-attacks-on-federated-learning/client-server-fl.svg\"\nalt=\"Typical Federated Learning network topology. The client, c_i, sends the gradient, \\nabla Q(\\theta_i), and/or weights, \\theta_i, of a particular iteration i. The central server then sends the updated model parameters \\theta_{i+1} initiating the next iteration.\" />\n<figcaption aria-hidden=\"true\">\nTypical Federated Learning network topology. The client,\n<span class=\"math inline\"><em>c</em><sub><em>i</em></sub></span>, sends\nthe gradient,\n<span class=\"math inline\">∇<em>Q</em>(<em>θ</em><sub><em>i</em></sub>)</span>,\nand/or weights,\n<span class=\"math inline\"><em>θ</em><sub><em>i</em></sub></span>, of a\nparticular iteration <span class=\"math inline\"><em>i</em></span>. The\ncentral server then sends the updated model parameters\n<span class=\"math inline\"><em>θ</em><sub><em>i</em> + 1</sub></span>\ninitiating the next iteration.\n</figcaption>\n</figure>\n\nVarious aggregation algorithms exist. The most popular of which are\n*Federated Stochastic Gradient Descent* (FedSGD) and *Federated\nAveraging* (FedAvg). These use the *gradient* (the function that\ndescribes how to optimize the model) or the aforementioned parameters,\n $\\theta$, of the client models respectively when communicating\nmodel updates Gu et al. (2022). While the technical details of these\nalgorithms are not crucial to this discussion, it is important to note\nthat both the gradient and the weights contain embedded information\nabout the client’s dataset (Fredrikson et al., 2015; Geiping et al.,\n2020).\n\n#### Vertical versus Horizontal FL\n\nLastly, there are two types of FL: Horizontal Federated Learning (HFL),\nor cross-device Federated Learning; and Vertical Federated Learning\n(VFL), or cross-silo Federated Learning (Abad et al., 2022; Suri et al.,\n2022). These differentiate in the way the client data is aligned to\nprovide a trainable model.\n\nIn the former, devices all collect data on the same features, but their\nsample space is not equal (the distributions might not align, and the\nsize can be different). This is the most used type of Federated\nLearning, which also is how companies such as Google train their models\non user data (McMahan & Ramage, 2017).\n\nIn the latter, we can imagine a set of hospitals or companies (or *data\nsilos*) that need to train a model on *all* the available user data. The\ndata, however, cannot be directly shared and the kind of data they\ncollect on their users is also different. Regardless, each user, or\n*subject*, is present in each database. They thus share the same sample\nspace but differ in the features they train their local models on.\n\n### Attacking Machine Learning\n\nThe machine learning approaches discussed so far (normal/*centralized*\nlearning and Federated Learning) contain several points at which an\nattacker could intervene to exploit various characteristics of the\nsystem. Before discussing inference attacks that take place in later\nstages of the machine learning pipeline, let us briefly discuss other\npotential threats to machine learning models.\n\nThe phases discussed in the first section (training, testing, and\ndeployment) correspond directly to different attacks which can be\ncategorized as follows (Chakraborty et al., 2018).\n\n1. *Poisoning Attack*: This type of attack, known as contamination of\n    the training data, takes place during the training time of the\n    machine learning model. An adversary tries to poison the training\n    data by injecting carefully designed samples to compromise the whole\n    learning process eventually.\n\n2. *Evasion Attack*: This is the most common type of attack in the\n    adversarial setting. The adversary tries to evade the system by\n    adjusting malicious samples during the testing phase. This setting\n    does not assume any influence over the training data.\n\n3. *Inference/Exploratory Attack*: These attacks do not influence any\n    dataset. Given black-box access to the model, they try to gain as\n    much knowledge as possible about the learning algorithm of the\n    underlying system and pattern in training data.\n\nWhile the first two also pose potential threats to the FL scheme and are\nvery popular in centralized machine learning, they are considerably\nharder to perform on Federated Learning as the data is distributed\n(Tolpegin et al., 2020). Databases of multiple clients have to be\ncompromised to create an exploit that is comparable to that of a\ncentralized machine-learning approach.\n\nInference attacks, however, threaten the privacy guarantees FL attempts\nto give. Inference attacks specifically try to *infer* information about\nthe dataset the model was trained on or the model itself. Thereby\nthreatening the confidentiality of the database, and thus the privacy,\nof the victims involved (Abad et al., 2022). Since they actively infer\ninformation about a deployed system, the amount of information on the\nsystem determines how powerful such an attack could be. For this reason,\nthey are also further specified as *white-box* or *black-box*, and\nsometimes *grey-box* inference attacks (Nasr et al., 2019).\n\n### Inference Attacks\n\nInference attacks can be applied to both centralized machine learning\nmodels and Federated Learning schemes. Many of the principles we will\ncover apply to both centralized and Federated Learning, but the focus\nwill be on applications on FL. Specifically, we will provide an overview\nof attack classifications as given by Abad et al. (2022).\n\nFirstly, depending on the target information the attacker attempts to\ninfer, the attack is classified as follows:\n\n- *Model Inversion*: In model inversion, the attacker attempts to invert\n  the machine learning model. Thereby finding the data point\n  corresponding to a certain label. Fredrikson et al. (2015) were able\n  to invert a facial recognition model, allowing them to recover the\n  image of an individual known to be in the training dataset.\n\n- *Membership Inference*: In this attack, the goal is to determine\n  whether a data point $(x, y)$ was part of the training set. In FL it\n  is also possible to determine whether a data point was part of the\n  training set of a particular client.\n\n- *Property Inference*: Property inference attempts to detect whether\n  the dataset used to train the (local) model contains some property\n  (e.g. whether images are noisy) (Ganju et al., 2018).\n\nSecondly, when considering a malicious central server, the attack can be\nclassified according to the manner in which the attacker interferes with\nthe training procedure (Abad et al., 2022):\n\n- *Passive*: Also known as an *honest-but-curious* scenario. The\n  attacker can only read or eavesdrop on communications (i.e. the\n  weights and gradient transmitted between the clients and the server),\n  and the local model and dataset in case of an honest-but-curious\n  client.\n\n- *Active*: The attack is more effective than the former but less\n  stealthy. It essentially changes the learning goal from minimizing\n  loss to maximizing inferred information from the victim.\n\nLastly, attacks can be categorized based on the position of the attacker\nin the network:\n\n- *Local*: The attacker is a client, i.e. they can only access *their*\n  database, parameters, and the global parameters they receive from the\n  server.\n\n- *Global*: The attacker is the central server. They do not have access\n  to any databases but can access the gradients/parameters sent by all\n  clients and the global model.\n\n## Inference Attacks in Federated Learning\n\nThis section will discuss the state-of-the-art of inference attacks on\nFederated Learning. Specifically, we will discuss progress in the field\nas of March 2022. The research presented here was found primarily by\nquerying Google Scholar with the terms “Inference Attacks on Federated\nLearning”, “Membership Inference on Federated Learning”, “Model\nInversion on Federated Learning”, and “Gradient Inversion on Federated\nLearning”. The next section will cover the threats these advances pose\nto current systems.\n\nFirst, we will discuss various attacks, focusing not only on their\nperformance but paying special attention to the scenario in which the\nresearchers placed the hypothetical adversary. Then, we will cover\ndefenses to some of these attacks.\n\n### Attacking\n\nVarious types of attacks fall under the umbrella of inference attacks.\nAs of the writing of this essay, the most popular are *Membership\nInference* and *Gradient Inversion* as these show the most results.\n*Passive* inference attacks are more often covered than their *active*\ncounterparts. For each paper, we annotate the type of attack (see\n[Inference Attacks](#inference-attacks)), summarize the findings of the\nauthors, and briefly discuss them.\n\n#### Do Gradient Inversion Attacks Make Federated Learning Unsafe?\n\nKeywords: *Model Inversion*, *Local/Global*, *Cross-Device/HFL*,\n*Passive*\n\nHatamizadeh et al. (2023) performed image reconstruction using gradient\ninversion while relaxing a strong assumption made in prior work\nregarding Batch Normalization (BN) (Ioffe & Szegedy, 2015). BN is a\ntechnique used in neural networks that significantly improve the\nlearning rate and stability, and is therefore ubiquitous in modern\nmachine learning. The technique introduces two learned parameters,\n$\\beta$ and $\\gamma$, which thus change during the learning\nprocess(Ioffe & Szegedy, 2015). Previous work has assumed these\nstatistics to be static (Geiping et al., 2020; Kaissis et al., 2021),\nintroducing an error that would compound over time. The authors were\nable to reliably reconstruct images without assuming static BN\nstatistics. The authors make a strong case for an inversion attack that\ncould be used in practice but still rely on priors (approximations of\nthe image) to make accurate reconstructions.\n\n#### Improved Gradient Inversion Attacks and Defenses in Federated Learning\n\nKeywords: *Membership Inference*, *Local/Global*, *Cross-Device/HFL*,\n*Passive*, *White-Box*\n\nGeng et al. (2023) proposed a framework for inverting both\n*FedAVG*-based and *FedSGD*-based networks in an “honest-but-curious”\nscenario. They mention prior work has failed to effectively perform\ngradient inversion when FL uses the *FedAVG* aggregation algorithm.\nFurthermore, they specify methods for fine-tuning the performance of\nimage restoration in the inverted model, allowing them to restore images\nthat were introduced 16 epochs before the current iteration. As\nFederated Learning is an iterative process, one can imagine that the\nfurther a data point is removed from the current iteration, the harder\nit is to infer from the current gradient. While their results are\npromising, they do assume a white-box attack scenario making their\nattack harder to perform.\n\n#### CS-MIA: Membership Inference Attack Based on Prediction Confidence Series in Federated Learning\n\nKeywords: *Membership Inference*, *Local/Global*, *Cross-Device/VFL*,\n*Passive*\n\nGu et al. (2022) were able to determine whether data points are members\nof certain datasets by following the trend in their classification\nconfidence. Over time, the global model should perform less well on\nparticipants’ private data, meaning that member data should follow a\ndifferent trend compared to non-member data. They then train a\nsupervised model the determine whether data points were part of the\ntraining set based on this assumption. The model is then used to\ndetermine the probability of unseen data being part of the target\ntraining data set. They show high accuracy and F1-scores for all\ndatasets with the lowest performer being MNIST (around 60% compared to\n\\>90% for the other datasets). Still, the proposed solution scores the\nbest out of all included approaches by a significant margin.\n\n#### Subject Membership Inference Attacks in Federated Learning\n\nKeywords: *Membership Inference*, *Local/Global*, *Cross-Silo/VFL*,\n*Passive*\n\nIn a black-box setting, Suri et al. (2022) propose a method for what\nthey call *Subject Inference* (see [Federated\nLearning](#federated-learning)). They describe previous work as being\ndisconnected from real-world scenarios as it (i) includes information\nadversaries would not normally have access to and (ii) assumes the\nadversary is looking for data points rather than individuals. Instead of\ndetermining whether one particular data point was part of the training\nset, the authors attempt to infer whether an individual, a *subject*,\n(or rather their distribution) is present in the dataset given some\npreexisting information on them. They show the attack to be very\neffective in various real-world datasets while also increasing the\nrealism of the scenario. They show Subject Inference to be a real threat\nto user privacy.\n\n#### Active Membership Inference Attack under Local Differential Privacy in Federated Learning\n\nKeywords: *Membership Inference*, *Local/Global*, *Cross-Device/HFL*,\n*Active*\n\nDifferent from other works, Nguyen et al. (2023) considers a maximally\nmalicious, i.e. *active*, membership inference attack. They implement a\nmethod for inferring membership of a particular data point in the\npresence of differential privacy (Dwork & Roth, 2013). Differential\nprivacy obscures the relation of the individual to the data point,\nwithout affecting the patterns used for training machine learning\nmodels. The authors show that their method performs well, even under\nsuch obscuring of the data. Furthermore, the attack only starts to\ndegrade after the level of obscurity interferes with model performance.\nThey show that more rigorous privacy methods should be proposed to deal\nwith such attacks.\n\n### Defending\n\nTo combat inference attacks, we discuss potential defenses against them.\nSome of the papers that are included have been discussed in the last\nsection. These have been marked with a footnote accordingly [^3]. For\neach paper, we will summarize the proposed measures and briefly discuss\nthem.\n\n#### Improved Gradient Inversion Attacks and Defenses in Federated Learning[^4]\n\nGeng et al. (2023) found that labels that only appeared only once were\nmore prone to their proposed inversion attacks (see\n[Attacks](#improved-gradient-inversion-attacks-and-defenses-in-federated-learning)).\nThey also mention the use of larger batch sizes in the global model\n(i.e. more clients) to reduce the amount of private information embedded\nin a single batch. Lastly, they claim FedAVG possesses “stronger privacy\npreserving capabilities than FedSGD”. As this was included in the\ndiscussion of their attack-oriented paper, they do not evaluate these\nclaims further.\n\n#### Do Gradient Inversion Attacks Make Federated Learning Unsafe?[^5]\n\nHatamizadeh et al. (2023) make several recommendations to make existing\nimplementations of FL safer, namely: (i) larger training sets, (ii)\nupdates from a larger number of iterations over different (iii) large\nbatch sizes. In addition, they mention three more changes that could\npotentially mitigate server-side (i.e. *Global*) gradient inversion\nattacks: (1) The use of *Homomorphic Encryption* (see [Future\nWork](#future-work)), (2) ensuring the attacker does not have knowledge\nof the model architecture, and (3) using an alternative aggregation\nalgorithm such as FedBN Andreux et al. (2020). The countermeasures\nprovided are relatively general. They also provided sources affirming\ntheir suspicions.\n\n#### An Empirical Analysis of Image Augmentation Against Model Inversion Attack in Federated Learning\n\nShin et al. (2023) propose the use of image augmentation as a more\nviable alternative to differential privacy (Dwork & Roth, 2013). Image\naugmentation is a data synthesis method that increases the size of the\ntraining set, and reduces over-fitting (Shorten & Khoshgoftaar, 2019).\nAs this introduces fake data while improving the overall performance of\nthe model, the authors suggest it could be used to mitigate model\ninversion attacks. The attack they used was introduced by (Geiping et\nal., 2020), and various more successful attacks have been constructed\nsince then Geng et al. (2023).\n\n#### ResSFL: A Resistance Transfer Framework for Defending Model Inversion Attack in Split Federated Learning\n\nIn a framework introduced by (J. Li et al., 2022), Split Federated\nLearning (SFL) (Annavaram & Avestimehr, n.d.) is augmented with a model\nthat attempts to invert the model before the client sends their model to\nthe central server. By choosing weights where the discriminator performs\npoorly, they claim to improve the resiliency of the scheme to model\ninversion attacks. They indeed show improvements over the standard\nimplementation of SFL but do not mention how this method compares to\nattacks on default FL.\n\n## Discussion\n\nIn this section, we will discuss the attacks and defenses as presented\nin the last section. Specifically, we determine the threats these\nattacks pose and if the defenses included could effectively mitigate\nthe. In the end, we propose new research directions that could help\nmitigate these threats.\n\n### Current Threats and Trends\n\nThe attacks presented show how Federated Learning might not be able to\nguarantee privacy. Privacy, thus, should still remain a concern even if\nFederated Learning brings stronger privacy guarantees than traditional\nmachine learning and its derivatives. Let us summarize the threats these\nattacks pose:\n\n- *More Realistic Scenarios*: Research starts to introduce more\n  realistic scenarios that could threaten current implementations of\n  Federated Learning. As the field matures, attacks seem to become more\n  realistic. Especially the work presented by Suri et al. (2022) poses a\n  real threat as it assumes a complete black-box attack with reasonable\n  assumptions while still showing good performance. Even in complete\n  black-box settings, however, we still assume the ability to intercept\n  and read the communications. Were this to be encrypted, such attacks\n  could possibly be mitigated (Y. Li et al., 2021).\n\n- *Increased Resilience Against Existing Privacy Measures*: Some of the\n  aforementioned papers has shown improvements concerning the evasion of\n  privacy-preserving measures. Nguyen et al. (2023) have shown how a\n  membership inference attack can be effectively performed in the\n  presence of differential privacy. Their method was effective to such a\n  degree that the attack was ineffective only once the privacy measures\n  started to affect model performance. The image augmentation\n  countermeasure proposed by (Shin et al., 2023) could be a viable\n  option. This countermeasure, however, was only tested in a *passive*\n  scenario.\n\n- *Stronger Attacks in Existing Scenarios*: As to be expected, some work\n  was focused on improving performance in existing scenarios. Gu et\n  al. (2022) have shown that much is still to be learned in the field by\n  proposing a relatively simple approach that improves upon all previous\n  methods by a large margin.\n\nSuch developments are not surprising, progress in both offense and\ndefense is to be expected. The speed at which research moves forward is\nvery impressive and suggests the field is still in the early stages of\ndevelopment. When considering using such new technologies in production,\nthis could be considered when assessing the security of such systems.\n\n### Future Work\n\nConsidering the aforementioned advances, the following directions could\nprovide useful for future research:\n\n1. Consider using existing preprocessing methods for privacy\n    preservation. Shin et al. (2023) and Hatamizadeh et al. (2023) both\n    either use or suggest using existing pre-processing or other\n    learning-enhancing augmentations to improve privacy. Efforts toward\n    generalizing data *before* training might prove a solution to both\n    overfitting and privacy.\n\n2. New attack methods would benefit from relaxing assumptions instead\n    of attempting to increase performance. By doing this, various of the\n    attacks shown have been to provide a more realistic view of the\n    privacy-preserving features of FL. While performance improvements\n    might provide interesting results and insights, focusing efforts on\n    exposing potential *realistic* threats would have a more direct\n    effect on our ability to assess FL from a privacy perspective.\n\n3. Working, safe Homomorphic Encryption (HE) would hamper most of the\n    aforementioned attacks. Being able to encrypt data *before* training\n    a model would make inference attacks completely benign (Lee et al.,\n    2022). Work from the past year, however, was able to infer\n    privacy-sensitive information about the training set regardless of\n    the presence of HE (Y. Li et al., 2021). More research on the robust\n    use of HE could prove a catch-all solution for many of the presented\n    machine-learning attacks.\n\n## Conclusion\n\nThis essay has provided an overview for security specialists and machine\nlearning specialists to assess the current state of Inference Attacks in\nFederated Learning. Progress over the last year has shown the field to\nbe advancing quickly. Introducing successful attacks on new, more\nrealistic scenarios, showing the ability to circumvent mature\nprivacy-preserving, measures and improving the performance of existing\nmethods. The developments seen here are concerning given the prevalence\nof Federated Learning. More research is needed to assess how\nprivacy-preserving Federated Learning actually is and whether additional\ncountermeasures provide enough security to circumvent the apparent\nthreats presented here.\n\n## References\n\n<div id=\"refs\" class=\"references csl-bib-body hanging-indent\"\nline-spacing=\"2\">\n\n<div id=\"ref-abadSecurityPrivacyFederated2022\" class=\"csl-entry\">\n\nAbad, G., Picek, S., Ramírez-Durán, V. J., & Urbieta, A. (2022). *On the\nSecurity & Privacy in Federated Learning* (No. arXiv:2112.05423). arXiv.\n<https://arxiv.org/abs/2112.05423>\n\n</div>\n\n<div id=\"ref-andreuxSiloedFederatedLearning2020\" class=\"csl-entry\">\n\nAndreux, M., Du Terrail, J. O., Beguier, C., & Tramel, E. W. (2020).\nSiloed Federated Learning for <span class=\"nocase\">Multi-centric\nHistopathology Datasets</span>. In S. Albarqouni, S. Bakas, K.\nKamnitsas, M. J. Cardoso, B. Landman, W. Li, F. Milletari, N. Rieke, H.\nRoth, D. Xu, & Z. Xu (Eds.), *Domain Adaptation and Representation\nTransfer, and Distributed and Collaborative Learning* (Vol. 12444, pp.\n129–139). Springer International Publishing.\n<https://doi.org/10.1007/978-3-030-60548-3_13>\n\n</div>\n\n<div id=\"ref-annavaramGroupKnowledgeTransfer\" class=\"csl-entry\">\n\nAnnavaram, C. H. M., & Avestimehr, S. (n.d.). *Group Knowledge Transfer:\nFederated Learning of Large CNNs at the Edge*.\n\n</div>\n\n<div id=\"ref-chakrabortyAdversarialAttacksDefences2018\"\nclass=\"csl-entry\">\n\nChakraborty, A., Alam, M., Dey, V., Chattopadhyay, A., & Mukhopadhyay,\nD. (2018). *Adversarial Attacks and Defences: A Survey* (No.\narXiv:1810.00069). arXiv. <https://arxiv.org/abs/1810.00069>\n\n</div>\n\n<div id=\"ref-dworkAlgorithmicFoundationsDifferential2013\"\nclass=\"csl-entry\">\n\nDwork, C., & Roth, A. (2013). The Algorithmic Foundations of\nDifferential Privacy. *Foundations and Trends in Theoretical Computer\nScience*, *9*(3-4), 211–407. <https://doi.org/10.1561/0400000042>\n\n</div>\n\n<div id=\"ref-fredriksonModelInversionAttacks2015\" class=\"csl-entry\">\n\nFredrikson, M., Jha, S., & Ristenpart, T. (2015). Model Inversion\nAttacks that Exploit Confidence Information and Basic Countermeasures.\n*Proceedings of the 22nd ACM SIGSAC Conference on Computer and\nCommunications Security*, 1322–1333.\n<https://doi.org/10.1145/2810103.2813677>\n\n</div>\n\n<div id=\"ref-ganjuPropertyInferenceAttacks2018\" class=\"csl-entry\">\n\nGanju, K., Wang, Q., Yang, W., Gunter, C. A., & Borisov, N. (2018).\nProperty Inference Attacks on Fully Connected Neural Networks using\nPermutation Invariant Representations. *Proceedings of the 2018 ACM\nSIGSAC Conference on Computer and Communications Security*, 619–633.\n<https://doi.org/10.1145/3243734.3243834>\n\n</div>\n\n<div id=\"ref-geipingInvertingGradientsHow2020\" class=\"csl-entry\">\n\nGeiping, J., Bauermeister, H., Dröge, H., & Moeller, M. (2020).\nInverting Gradients - How easy is it to break privacy in federated\nlearning? *Advances in Neural Information Processing Systems*, *33*,\n16937–16947.\n\n</div>\n\n<div id=\"ref-gengImprovedGradientInversion2023\" class=\"csl-entry\">\n\nGeng, J., Mou, Y., Li, Q., Li, F., Beyan, O., Decker, S., & Rong, C.\n(2023). Improved Gradient Inversion Attacks and Defenses in Federated\nLearning. *IEEE Transactions on Big Data*, 1–13.\n<https://doi.org/10.1109/TBDATA.2023.3239116>\n\n</div>\n\n<div id=\"ref-guCSMIAMembershipInference2022\" class=\"csl-entry\">\n\nGu, Y., Bai, Y., & Xu, S. (2022). CS-MIA: Membership inference attack\nbased on prediction confidence series in federated learning. *Journal of\nInformation Security and Applications*, *67*, 103201.\n<https://doi.org/10.1016/j.jisa.2022.103201>\n\n</div>\n\n<div id=\"ref-hatamizadehGradientInversionAttacks2023\" class=\"csl-entry\">\n\nHatamizadeh, A., Yin, H., Molchanov, P., Myronenko, A., Li, W., Dogra,\nP., Feng, A., Flores, M. G., Kautz, J., Xu, D., & Roth, H. R. (2023). Do\nGradient Inversion Attacks Make Federated Learning Unsafe? *IEEE\nTransactions on Medical Imaging*, 1–1.\n<https://doi.org/10.1109/TMI.2023.3239391>\n\n</div>\n\n<div id=\"ref-ioffeBatchNormalizationAccelerating2015\" class=\"csl-entry\">\n\nIoffe, S., & Szegedy, C. (2015). *Batch Normalization: Accelerating Deep\nNetwork Training by Reducing Internal Covariate Shift* (No.\narXiv:1502.03167). arXiv. <https://doi.org/10.48550/arXiv.1502.03167>\n\n</div>\n\n<div id=\"ref-kaissisEndtoendPrivacyPreserving2021\" class=\"csl-entry\">\n\nKaissis, G., Ziller, A., Passerat-Palmbach, J., Ryffel, T., Usynin, D.,\nTrask, A., Lima, I., Mancuso, J., Jungmann, F., Steinborn, M.-M., Saleh,\nA., Makowski, M., Rueckert, D., & Braren, R. (2021). End-to-end privacy\npreserving deep learning on multi-institutional medical imaging. *Nature\nMachine Intelligence*, *3*(6), 473–484.\n<https://doi.org/10.1038/s42256-021-00337-8>\n\n</div>\n\n<div id=\"ref-konecnyFederatedLearningStrategies2017\" class=\"csl-entry\">\n\nKonečný, J., McMahan, H. B., Yu, F. X., Richtárik, P., Suresh, A. T., &\nBacon, D. (2017). *Federated Learning: Strategies for Improving\nCommunication Efficiency* (No. arXiv:1610.05492). arXiv.\n<https://doi.org/10.48550/arXiv.1610.05492>\n\n</div>\n\n<div id=\"ref-kononenkoMachineLearningMedical2001\" class=\"csl-entry\">\n\nKononenko, I. (2001). Machine learning for medical diagnosis: History,\nstate of the art and perspective. *Artificial Intelligence in Medicine*,\n*23*(1), 89–109. <https://doi.org/10.1016/S0933-3657(01)00077-X>\n\n</div>\n\n<div id=\"ref-leePrivacyPreservingMachineLearning2022\" class=\"csl-entry\">\n\nLee, J.-W., Kang, H., Lee, Y., Choi, W., Eom, J., Deryabin, M., Lee, E.,\nLee, J., Yoo, D., Kim, Y.-S., & No, J.-S. (2022). Privacy-Preserving\nMachine Learning With Fully Homomorphic Encryption for Deep Neural\nNetwork. *IEEE Access*, *10*, 30039–30054.\n<https://doi.org/10.1109/ACCESS.2022.3159694>\n\n</div>\n\n<div id=\"ref-liResSFLResistanceTransfer2022\" class=\"csl-entry\">\n\nLi, J., Rakin, A. S., Chen, X., He, Z., Fan, D., & Chakrabarti, C.\n(2022). ResSFL: A Resistance Transfer Framework for Defending Model\nInversion Attack in Split Federated Learning. *Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition*,\n10194–10202.\n\n</div>\n\n<div id=\"ref-liFedBNFederatedLearning2021\" class=\"csl-entry\">\n\nLi, X., Jiang, M., Zhang, X., Kamp, M., & Dou, Q. (2021). *FedBN:\nFederated Learning on Non-IID Features via Local Batch Normalization*\n(No. arXiv:2102.07623). arXiv.\n<https://doi.org/10.48550/arXiv.2102.07623>\n\n</div>\n\n<div id=\"ref-liPrivacyThreatsAnalysis2021\" class=\"csl-entry\">\n\nLi, Y., Bao, Y., Xiang, L., Liu, J., Chen, C., Wang, L., & Wang, X.\n(2021). *Privacy Threats Analysis to Secure Federated Learning* (No.\narXiv:2106.13076). arXiv. <https://arxiv.org/abs/2106.13076>\n\n</div>\n\n<div id=\"ref-liuSummaryChatGPTGPT42023\" class=\"csl-entry\">\n\nLiu, Y., Han, T., Ma, S., Zhang, J., Yang, Y., Tian, J., He, H., Li, A.,\nHe, M., Liu, Z., Wu, Z., Zhu, D., Li, X., Qiang, N., Shen, D., Liu, T.,\n& Ge, B. (2023). Summary of ChatGPT/GPT-4 Research and Perspective\nTowards the Future of Large Language Models. In *arXiv.org*.\n<https://arxiv.org/abs/2304.01852v3>.\n\n</div>\n\n<div id=\"ref-mcmahanFederatedLearningCollaborative2017\"\nclass=\"csl-entry\">\n\nMcMahan, B., & Ramage, D. (2017). *Federated Learning: Collaborative\nMachine Learning without Centralized Training Data*.\n<https://ai.googleblog.com/2017/04/federated-learning-collaborative.html>.\n\n</div>\n\n<div id=\"ref-nasrComprehensivePrivacyAnalysis2019\" class=\"csl-entry\">\n\nNasr, M., Shokri, R., & Houmansadr, A. (2019). Comprehensive Privacy\nAnalysis of Deep Learning: Passive and <span class=\"nocase\">Active\nWhite-box Inference Attacks</span> against Centralized and Federated\nLearning. *2019 IEEE Symposium on Security and Privacy (SP)*, 739–753.\n<https://doi.org/10.1109/SP.2019.00065>\n\n</div>\n\n<div id=\"ref-nguyenActiveMembershipInference2023\" class=\"csl-entry\">\n\nNguyen, T., Lai, P., Tran, K., Phan, N., & Thai, M. T. (2023). *Active\nMembership Inference Attack under Local Differential Privacy in\nFederated Learning* (No. arXiv:2302.12685). arXiv.\n<https://doi.org/10.48550/arXiv.2302.12685>\n\n</div>\n\n<div id=\"ref-opreaAdversarialMachineLearning2023\" class=\"csl-entry\">\n\nOprea, A., & Vassilev, A. (2023). *Adversarial Machine Learning: A\nTaxonomy and Terminology of Attacks and Mitigations (Draft)* (NIST AI\n100-2e2023 ipd). National Institute of Standards and Technology.\n\n</div>\n\n<div id=\"ref-riekeFutureDigitalHealth2020\" class=\"csl-entry\">\n\nRieke, N., Hancox, J., Li, W., Milletarì, F., Roth, H. R., Albarqouni,\nS., Bakas, S., Galtier, M. N., Landman, B. A., Maier-Hein, K., Ourselin,\nS., Sheller, M., Summers, R. M., Trask, A., Xu, D., Baust, M., &\nCardoso, M. J. (2020). The future of digital health with federated\nlearning. *NPJ Digital Medicine*, *3*, 119.\n<https://doi.org/10.1038/s41746-020-00323-1>\n\n</div>\n\n<div id=\"ref-shinEmpiricalAnalysisImage2023\" class=\"csl-entry\">\n\nShin, S., Boyapati, M., Suo, K., Kang, K., & Son, J. (2023). An\nempirical analysis of image augmentation against model inversion attack\nin federated learning. *Cluster Computing*, *26*(1), 349–366.\n<https://doi.org/10.1007/s10586-022-03596-1>\n\n</div>\n\n<div id=\"ref-shortenSurveyImageData2019\" class=\"csl-entry\">\n\nShorten, C., & Khoshgoftaar, T. M. (2019). A survey on Image Data\nAugmentation for Deep Learning. *Journal of Big Data*, *6*(1), 60.\n<https://doi.org/10.1186/s40537-019-0197-0>\n\n</div>\n\n<div id=\"ref-suriSubjectMembershipInference2022\" class=\"csl-entry\">\n\nSuri, A., Kanani, P., Marathe, V. J., & Peterson, D. W. (2022). Subject\nMembership Inference Attacks in Federated Learning. In *arXiv.org*.\n<https://arxiv.org/abs/2206.03317v3>.\n\n</div>\n\n<div id=\"ref-tolpeginDataPoisoningAttacks2020\" class=\"csl-entry\">\n\nTolpegin, V., Truex, S., Gursoy, M. E., & Liu, L. (2020). Data Poisoning\nAttacks Against Federated Learning Systems. In *arXiv.org*.\n<https://arxiv.org/abs/2007.08432v2>.\n\n</div>\n\n</div>\n\n[^1]: More types of machine learning exist (Oprea & Vassilev, 2023), but\n    the details of their taxonomy are not important for this discussion.\n    The focus will be on Federated Learning which, almost exclusively\n    uses supervised learning.\n\n[^2]: Other network topologies are possible within Federated Learning.\n    One notable example is a peer-to-peer network which allows for a\n    completely decentralized machine-learning approach. Most actual\n    implemented systems, however, have a normal client-server structure\n    (Abad et al., 2022).\n\n[^3]: Often, novel attack proposals also include possible\n    countermeasures. Some of the papers covered in the last section,\n    therefore, have also been included in this section.\n\n[^4]: Often, novel attack proposals also include possible\n    countermeasures. Some of the papers covered in the last section,\n    therefore, have also been included in this section.\n\n[^5]: Often, novel attack proposals also include possible\n    countermeasures. Some of the papers covered in the last section,\n    therefore, have also been included in this section.\n"},"__N_SSG":true}